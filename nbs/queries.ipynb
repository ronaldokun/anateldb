{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp query\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queries\n",
    "\n",
    "> Este módulo executa as queries sql / MongoDB necessárias para baixar os dados do STEL, RADCOM e MOSAICO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import requests\n",
    "import re\n",
    "from decimal import *\n",
    "from typing import *\n",
    "from gazpacho import Soup\n",
    "from rich.progress import track\n",
    "from pathlib import Path\n",
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "import pandas_read_xml as pdx\n",
    "import pyodbc\n",
    "import collections\n",
    "from fastcore.utils import listify\n",
    "from fastcore.foundation import L\n",
    "from fastcore.test import *\n",
    "from anateldb.constants import *\n",
    "from pyarrow import ArrowInvalid\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "getcontext().prec = 5\n",
    "\n",
    "BW = {'H': 0.001, 'K': 1, 'M': 1000, 'G': 1000000}\n",
    "BW_pattern = re.compile(\"^(\\d{1,3})([HKMG])(\\d{0,2})$\")\n",
    "BW_MAP = {'230': '256K', '231': '256K', '205': '10K0', '800': '6M00', '801': '5M70', '248': '6M00', '247': '5M70', '167': '6M00'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasta = Path('c:/Users/rsilva/db')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otimização dos Tipos de dados\n",
    "A serem criados dataframes, normalmente a tipo de data é aquele com maior resolução possível, nem sempre isso é necessário, os arquivos de espectro mesmo possuem somente uma casa decimal, portanto um `float16` já é suficiente para armazená-los. As funções a seguir fazem essa otimização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below borrowed from https://medium.com/bigdatarepublic/advanced-pandas-optimize-speed-and-memory-a654b53be6c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def optimize_floats(df: pd.DataFrame, exclude=None) -> pd.DataFrame:\n",
    "    floats = df.select_dtypes(include=[\"float64\"]).columns.tolist()\n",
    "    floats = [c for c in floats if c not in listify(exclude)]\n",
    "    df[floats] = df[floats].apply(pd.to_numeric, downcast=\"float\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def optimize_ints(df: pd.DataFrame, exclude=None) -> pd.DataFrame:\n",
    "    ints = df.select_dtypes(include=[\"int64\"]).columns.tolist()\n",
    "    ints = [c for c in ints if c not in listify(exclude)]\n",
    "    df[ints] = df[ints].apply(pd.to_numeric, downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def optimize_objects(\n",
    "    df: pd.DataFrame, datetime_features: List[str], exclude=None\n",
    ") -> pd.DataFrame:\n",
    "    for col in df.select_dtypes(include=[\"object\"]).columns.tolist():\n",
    "        if col not in datetime_features:\n",
    "            if col in listify(exclude):\n",
    "                continue\n",
    "            num_unique_values = len(df[col].unique())\n",
    "            num_total_values = len(df[col])\n",
    "            if float(num_unique_values) / num_total_values < 0.5:\n",
    "                dtype = \"category\"\n",
    "            else:\n",
    "                dtype = \"string\"\n",
    "            df[col] = df[col].astype(dtype)\n",
    "        else:\n",
    "            df[col] = pd.to_datetime(df[col]).dt.date\n",
    "    return df\n",
    "\n",
    "\n",
    "def df_optimize(df: pd.DataFrame, datetime_features: List[str] = None, exclude=None):\n",
    "    if datetime_features is None:\n",
    "        datetime_features = []\n",
    "    return optimize_floats(\n",
    "        optimize_ints(optimize_objects(df, datetime_features, exclude), exclude),\n",
    "        exclude,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conexão com o banco de dados\n",
    "A função a seguir é um `wrapper` simples que utiliza o `pyodbc` para se conectar ao banco de dados base da Anatel e retorna o objeto da conexão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def connect_db():\n",
    "    \"\"\"Conecta ao Banco ANATELBDRO01 e retorna o 'cursor' (iterador) do Banco pronto para fazer iterações\"\"\"\n",
    "    return pyodbc.connect(\n",
    "        \"Driver={ODBC Driver 17 for SQL Server};\"\n",
    "        \"Server=ANATELBDRO01;\"\n",
    "        \"Database=SITARWEB;\"\n",
    "        \"Trusted_Connection=yes;\"\n",
    "        \"MultipleActiveResultSets=True;\",\n",
    "        timeout=TIMEOUT,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "def test_connection():\n",
    "    conn = connect_db()\n",
    "    cursor = conn.cursor()\n",
    "    for query in (RADCOM, STEL):\n",
    "        cursor.execute(query)\n",
    "        test_eq(type(cursor.fetchone()), pyodbc.Row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções auxiliares de Formatação\n",
    "As funções a seguir são utilizadas para formatar diversos objetos intermediários utilizados nas funções de leitura e atualização da base de dados e não são chamadas diretamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def row2dict(row):\n",
    "    \"\"\"Receives a json row and return the dictionary from it\"\"\"\n",
    "    return dict(row.items())\n",
    "\n",
    "\n",
    "def dict2cols(df, reject=()):\n",
    "    \"\"\"Recebe um dataframe com dicionários nas células e extrai os dicionários como colunas\n",
    "    Opcionalmente ignora e exclue as colunas em reject\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        if column in reject:\n",
    "            df.drop(column, axis=1, inplace=True)\n",
    "            continue\n",
    "        if type(df[column].iloc[0]) == collections.OrderedDict:\n",
    "            try:\n",
    "                new_df = pd.DataFrame(df[column].apply(row2dict).tolist())\n",
    "                df = pd.concat([df, new_df], axis=1)\n",
    "                df.drop(column, axis=1, inplace=True)\n",
    "            except AttributeError:\n",
    "                continue\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_plano_basico(row, cols=COL_PB):\n",
    "    \"\"\"Receives a json row and filter the column in `cols`\"\"\"\n",
    "    return {k: row[k] for k in cols}\n",
    "\n",
    "\n",
    "def scrape_dataframe(id_list):\n",
    "    df = pd.DataFrame()\n",
    "    for id_ in track(id_list, description=\"Baixando informações complementares da Web\"):\n",
    "        html = requests.get(ESTACAO.format(id_))\n",
    "        df = df.append(pd.read_html(Soup(html.text).find(\"table\").html)[0])\n",
    "\n",
    "    df.rename(\n",
    "        columns={\"NumFistel\": \"Fistel\", \"Num Serviço\": \"Num_Serviço\"}, inplace=True\n",
    "    )\n",
    "    return df[\n",
    "        [\n",
    "            \"Status\",\n",
    "            \"Entidade\",\n",
    "            \"Fistel\",\n",
    "            \"Frequência\",\n",
    "            \"Classe\",\n",
    "            \"Num_Serviço\",\n",
    "            \"Município\",\n",
    "            \"UF\",\n",
    "        ]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def input_coordenates(df, pasta):\n",
    "    \"\"\"Input the NA's in Coordinates with those of the cities\"\"\"\n",
    "    municipios = Path(f\"{pasta}/municípios.fth\")\n",
    "    if not municipios.exists():\n",
    "        municipios = Path(f\"{pasta}/municípios.xlsx\")\n",
    "        if not municipios.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"É necessario a tabela de municípios municípios.fth | municípios.xlsx na pasta {pasta}\"\n",
    "            )\n",
    "        m = pd.read_excel(municipios, engine=\"openpyxl\")\n",
    "    else:\n",
    "        m = pd.read_feather(municipios)\n",
    "    m.loc[\n",
    "        m.Município == \"Sant'Ana do Livramento\", \"Município\"\n",
    "    ] = \"Santana do Livramento\"\n",
    "    m[\"Município\"] = m.Município.apply(unidecode).str.lower().str.replace(\"'\", \" \")\n",
    "    m[\"UF\"] = m.UF.str.lower()\n",
    "    df[\"Coordenadas_do_Município\"] = False\n",
    "    df[\"Latitude\"] = df.Latitude.str.replace(\",\", \".\")\n",
    "    df[\"Longitude\"] = df.Longitude.str.replace(\",\", \".\")\n",
    "    df.loc[df[\"Município\"] == \"Poxoréo\", \"Município\"] = \"Poxoréu\"\n",
    "    df.loc[df[\"Município\"] == \"Couto de Magalhães\", \"Município\"] = \"Couto Magalhães\"\n",
    "    df[\"Município\"] = df.Município.astype(\"string\")\n",
    "    criteria = (\n",
    "        (df.Latitude == \"\")\n",
    "        | (df.Latitude.isna())\n",
    "        | (df.Longitude == \"\")\n",
    "        | (df.Longitude.isna())\n",
    "    ) & df.Município.isna()\n",
    "    df = df[~criteria]\n",
    "    for row in df[\n",
    "        (\n",
    "            (df.Latitude == \"\")\n",
    "            | (df.Latitude.isna())\n",
    "            | (df.Longitude == \"\")\n",
    "            | (df.Longitude.isna())\n",
    "        )\n",
    "    ].itertuples():\n",
    "        try:\n",
    "            left = unidecode(row.Município).lower()\n",
    "            m_coord = (\n",
    "                m.loc[\n",
    "                    (m.Município == left) & (m.UF == row.UF.lower()),\n",
    "                    [\"Latitude\", \"Longitude\"],\n",
    "                ]\n",
    "                .values.flatten()\n",
    "                .tolist()\n",
    "            )\n",
    "            df.loc[row.Index, \"Latitude\"] = m_coord[0]\n",
    "            df.loc[row.Index, \"Longitude\"] = m_coord[1]\n",
    "            df.loc[row.Index, \"Coordenadas_do_Município\"] = True\n",
    "        except ValueError:\n",
    "            print(left, row.UF, m_coord)\n",
    "            continue\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_merge(pasta, df):\n",
    "    df = df.copy()\n",
    "    COLS = [c for c in df.columns if \"_x\" in c]\n",
    "    for col in COLS:\n",
    "        col_x = col\n",
    "        col_y = col.split(\"_\")[0] + \"_y\"\n",
    "        if df[col_x].count() > df[col_y].count():\n",
    "            a, b = col_x, col_y\n",
    "        else:\n",
    "            a, b = col_y, col_x\n",
    "\n",
    "        df.loc[df[a].isna(), a] = df.loc[df[a].isna(), b]\n",
    "        df.drop(b, axis=1, inplace=True)\n",
    "        df.rename({a: a[:-2]}, axis=1, inplace=True)\n",
    "\n",
    "    df.loc[df.Latitude_Transmissor == \"\", \"Latitude_Transmissor\"] = df.loc[\n",
    "        df.Latitude_Transmissor == \"\", \"Latitude_Estação\"\n",
    "    ]\n",
    "    df.loc[df.Longitude_Transmissor == \"\", \"Longitude_Transmissor\"] = df.loc[\n",
    "        df.Longitude_Transmissor == \"\", \"Longitude_Estação\"\n",
    "    ]\n",
    "    df.loc[df.Latitude_Transmissor.isna(), \"Latitude_Transmissor\"] = df.loc[\n",
    "        df.Latitude_Transmissor.isna(), \"Latitude_Estação\"\n",
    "    ]\n",
    "    df.loc[df.Longitude_Transmissor.isna(), \"Longitude_Transmissor\"] = df.loc[\n",
    "        df.Longitude_Transmissor.isna(), \"Longitude_Estação\"\n",
    "    ]\n",
    "    df.drop([\"Latitude_Estação\", \"Longitude_Estação\"], axis=1, inplace=True)\n",
    "    df.rename(\n",
    "        columns={\n",
    "            \"Latitude_Transmissor\": \"Latitude\",\n",
    "            \"Longitude_Transmissor\": \"Longitude\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    df = input_coordenates(df, pasta)\n",
    "\n",
    "    df[\"Frequência\"] = df.Frequência.str.replace(\",\", \".\")\n",
    "\n",
    "    if freq_nans := df[df.Frequência.isna()].Id.tolist():\n",
    "        complement_df = scrape_dataframe(freq_nans)\n",
    "        df.loc[\n",
    "            df.Frequência.isna(),\n",
    "            [\n",
    "                \"Status\",\n",
    "                \"Entidade\",\n",
    "                \"Fistel\",\n",
    "                \"Frequência\",\n",
    "                \"Classe\",\n",
    "                \"Num_Serviço\",\n",
    "                \"Município\",\n",
    "                \"UF\",\n",
    "            ],\n",
    "        ] = complement_df.values\n",
    "\n",
    "    for r in df[(df.Entidade.isna()) | (df.Entidade == \"\")].itertuples():\n",
    "        df.loc[r.Index, \"Entidade\"] = ENTIDADES.get(r.Fistel, \"\")\n",
    "\n",
    "    df.loc[df[\"Número_da_Estação\"] == \"\", \"Número_da_Estação\"] = -1\n",
    "    df[\"Latitude\"] = df[\"Latitude\"].astype(\"float\")\n",
    "    df[\"Longitude\"] = df[\"Longitude\"].astype(\"float\")\n",
    "    df[\"Frequência\"] = df.Frequência.astype(\"float\")\n",
    "    df.loc[df.Serviço == \"OM\", \"Frequência\"] = df.loc[\n",
    "        df.Serviço == \"OM\", \"Frequência\"\n",
    "    ].apply(lambda x: Decimal(x) / Decimal(1000))\n",
    "    df[\"Frequência\"] = df.Frequência.astype(\"float\")\n",
    "    df[\"Validade_RF\"] = df.Validade_RF.astype(\"string\").str.slice(0, 10)\n",
    "    df.loc[df[\"Num_Ato\"] == \"\", \"Num_Ato\"] = -1\n",
    "    df[\"Num_Ato\"] = df.Num_Ato.astype(\"string\")\n",
    "    df[\"Num_Serviço\"] = df.Num_Serviço.astype(\"category\")\n",
    "    return df_optimize(df, exclude=[\"Frequência\"])\n",
    "\n",
    "def parse_bw(bw):\n",
    "    if match := re.match(BW_pattern, bw):\n",
    "        multiplier = BW[match.group(2)]\n",
    "        if mantissa := match.group(3):\n",
    "            number = float(f'{match.group(1)}.{mantissa}')\n",
    "        else:\n",
    "            number = float(match.group(1))\n",
    "        return multiplier * number\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de Leitura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def read_estações(path):\n",
    "    def extrair_ato(row):\n",
    "        if not isinstance(row, str):\n",
    "            row = listify(row)[::-1]\n",
    "            for d in row:\n",
    "                if not isinstance(d, dict):\n",
    "                    continue\n",
    "                if (d.get(\"@TipoDocumento\") == \"Ato\") and (\n",
    "                    d.get(\"@Razao\") == \"Autoriza o Uso de Radiofrequência\"\n",
    "                ):\n",
    "                    return d[\"@NumDocumento\"], d[\"@DataDOU\"][:10]\n",
    "            return \"\", \"\"\n",
    "        return \"\", \"\"\n",
    "\n",
    "    es = pdx.read_xml(path, [\"estacao_rd\"])\n",
    "    dfs = []\n",
    "    for i in range(es.shape[0]):\n",
    "        df = pd.DataFrame(es[\"row\"][i]).replace(\"\", pd.NA)\n",
    "        df = dict2cols(df)\n",
    "        df.columns = [unidecode(c).lower().replace(\"@\", \"\") for c in df.columns]\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs)\n",
    "    df = df[df.state.str.contains(\"-C1$|-C2$|-C3$|-C4$|-C7|-C98$\")].reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    docs = L(df.historico_documentos.apply(extrair_ato).tolist())\n",
    "    df = df.loc[:, COL_ESTACOES]\n",
    "    df[\"Num_Ato\"] = docs.itemgot(0).map(str)\n",
    "    df[\"Data_Ato\"] = docs.itemgot(1).map(str)\n",
    "    df.columns = NEW_ESTACOES\n",
    "    df[\"Validade_RF\"] = df.Validade_RF.astype(\"string\").str.slice(0, 10)\n",
    "    df[\"Data_Ato\"] = df.Data_Ato.str.slice(0, 10)\n",
    "    for c in df.columns:\n",
    "        df.loc[df[c] == \"\", c] = pd.NA\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_plano_basico(path):\n",
    "    pb = pdx.read_xml(path, [\"plano_basico\"])\n",
    "    dfs = []\n",
    "    for i in range(pb.shape[0]):\n",
    "        df = pd.DataFrame(pb[\"row\"][i]).replace(\"\", pd.NA)\n",
    "        df = dict2cols(df)\n",
    "        df.columns = [unidecode(c).lower().replace(\"@\", \"\") for c in df.columns]\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs)\n",
    "    df = df.loc[df.pais == \"BRA\", COL_PB].reset_index(drop=True)\n",
    "    for c in df.columns:\n",
    "        df.loc[df[c] == \"\", c] = pd.NA\n",
    "    df.columns = NEW_PB\n",
    "    df.sort_values([\"Id\", \"Canal\"], inplace=True)\n",
    "    ENTIDADES.update(\n",
    "        {r.Fistel: r.Entidade for r in df.itertuples() if str(r.Entidade) == \"<NA>\"}\n",
    "    )\n",
    "    df = df.groupby(\"Id\", as_index=False).first()  # remove duplicated with NaNs\n",
    "    df.dropna(subset=[\"Status\"], inplace=True)\n",
    "    df = df[df.Status.str.contains(\"-C1$|-C2$|-C3$|-C4$|-C7|-C98$\")].reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atualização das bases de dados\n",
    "As bases de dados são atualizadas atráves das funções a seguir, o único argumento passado em todas elas é a pasta na qual os arquivos locais processados serão salvos, os nomes dos arquivos são padronizados e não podem ser editados para que as funções de leitura e processamento recebam somente a pasta na qual esses arquivos foram salvos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def update_radcom(pasta):\n",
    "    \"\"\"Atualiza a tabela local retornada pela query `RADCOM`\"\"\"\n",
    "    with console.status(\n",
    "        \"[cyan]Lendo o Banco de Dados de Radcom...\", spinner=\"earth\"\n",
    "    ) as status:\n",
    "        try:\n",
    "            conn = connect_db()\n",
    "            df = pd.read_sql_query(RADCOM, conn)\n",
    "            df[\"Unidade\"] = \"MHz\"\n",
    "            df = df_optimize(df, exclude=[\"Frequência\"])\n",
    "            try:\n",
    "                df.to_feather(f\"{pasta}/radcom.fth\")\n",
    "            except ArrowInvalid:\n",
    "                Path(f\"{pasta}/radcom.fth\").unlink()\n",
    "                df.to_excel(f\"{pasta}/radcom.xlsx\", engine=\"openpyxl\", index=False)\n",
    "        except pyodbc.OperationalError:\n",
    "            status.console.log(\n",
    "                \"Não foi possível abrir uma conexão com o SQL Server. Esta conexão somente funciona da rede cabeada!\"\n",
    "            )\n",
    "\n",
    "\n",
    "def update_stel(pasta):\n",
    "    \"\"\"Atualiza a tabela local retornada pela query `STEL`\"\"\"\n",
    "    with console.status(\n",
    "        \"[magenta]Lendo o Banco de Dados do STEL. Processo Lento, aguarde...\",\n",
    "        spinner=\"moon\",\n",
    "    ) as status:\n",
    "        try:\n",
    "            conn = connect_db()\n",
    "            df = pd.read_sql_query(STEL, conn)\n",
    "            df[\"Validade_RF\"] = df.Validade_RF.astype(\"str\").str.slice(0, 10)\n",
    "            df[\"Num_Serviço\"] = df.Num_Serviço.astype(\"category\")\n",
    "            df.loc[df.Unidade == \"kHz\", \"Frequência\"] = df.loc[\n",
    "                df.Unidade == \"kHz\", \"Frequência\"\n",
    "            ].apply(lambda x: Decimal(x) / Decimal(1000))\n",
    "            df.loc[df.Unidade == \"GHz\", \"Frequência\"] = df.loc[\n",
    "                df.Unidade == \"GHz\", \"Frequência\"\n",
    "            ].apply(lambda x: Decimal(x) * Decimal(1000))\n",
    "            df[\"Frequência\"] = df.Frequência.astype(\"float\")\n",
    "            df.loc[df.Unidade == \"kHz\", \"Unidade\"] = \"MHz\"\n",
    "            df = df_optimize(df, exclude=[\"Frequência\"])\n",
    "            try:\n",
    "                df.to_feather(f\"{pasta}/stel.fth\")\n",
    "            except ArrowInvalid:\n",
    "                Path(f\"{pasta}/stel.fth\").unlink()\n",
    "                df.to_excel(f\"{pasta}/stel.xlsx\", engine=\"openpyxl\", index=False)\n",
    "        except pyodbc.OperationalError:\n",
    "            status.console.log(\n",
    "                \"Não foi possível abrir uma conexão com o SQL Server. Esta conexão somente funciona da rede cabeada!\"\n",
    "            )\n",
    "\n",
    "\n",
    "def update_mosaico(pasta):\n",
    "    \"\"\"Atualiza a tabela local do Mosaico. É baixado e processado arquivos xml zipados da página pública do Spectrum E\"\"\"\n",
    "    with console.status(\n",
    "        \"[blue]Baixando as Estações do Mosaico...\", spinner=\"shark\"\n",
    "    ) as status:\n",
    "        file = requests.get(ESTACOES)\n",
    "        with open(f\"{pasta}/estações.zip\", \"wb\") as estações:\n",
    "            estações.write(file.content)\n",
    "    with console.status(\n",
    "        \"[blue]Baixando o Plano Básico das Estações...\", spinner=\"weather\"\n",
    "    ) as status:\n",
    "        file = requests.get(PLANO_BASICO)\n",
    "        with open(f\"{pasta}/Canais.zip\", \"wb\") as plano_basico:\n",
    "            plano_basico.write(file.content)\n",
    "    console.print(\":package: [blue]Consolidando as bases de dados...\")\n",
    "    estações = read_estações(f\"{pasta}/estações.zip\")\n",
    "    plano_basico = read_plano_basico(f\"{pasta}/Canais.zip\")\n",
    "    df = estações.merge(plano_basico, on=\"Id\", how=\"left\")\n",
    "    df[\"Número_da_Estação\"] = df[\"Número_da_Estação\"].fillna(-1)\n",
    "    df[\"Número_da_Estação\"] = df[\"Número_da_Estação\"].astype(\"int\")\n",
    "    df = clean_merge(pasta, df)\n",
    "    try:\n",
    "        df.reset_index(drop=True).to_feather(f\"{pasta}/mosaico.fth\")\n",
    "    except ArrowInvalid:\n",
    "        Path(f\"{pasta}/mosaico.fth\").unlink()\n",
    "        with pd.ExcelWriter(f\"{pasta}/mosaico.xlsx\") as workbook:\n",
    "            df.reset_index(drop=True).to_excel(\n",
    "                workbook, sheet_name=\"Sheet1\", engine=\"openpyxl\", index=False\n",
    "            )\n",
    "    console.print(\"Kbô :vampire:\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def _merge_df(df1, df2, on, how=\"left\"):\n",
    "    \"\"\"Merge two dataframes with the same columns and records\"\"\"\n",
    "    df = pd.merge(df1, df2, on=on, how=how)\n",
    "    x = df.Description_x.notna()\n",
    "    y = df.Description_y.notna()\n",
    "    df.loc[x & y, \"Description\"] = (\n",
    "        df.loc[x & y, \"Description_x\"] + \" | \" + df.loc[x & y, \"Description_y\"]\n",
    "    )\n",
    "    df.loc[x & ~y, \"Description\"] = df.loc[x & ~y, \"Description_x\"]\n",
    "    df.loc[~x & y, \"Description\"] = df.loc[~x & y, \"Description_y\"]\n",
    "    df.loc[x & y, \"Latitude\"] = (\n",
    "        df.loc[x & y, \"Latitude_x\"] + df.loc[x & y, \"Latitude_y\"]\n",
    "    ) / 2\n",
    "    df.loc[x & y, \"Longitude\"] = (\n",
    "        df.loc[x & y, \"Longitude_x\"] + df.loc[x & y, \"Longitude_y\"]\n",
    "    ) / 2\n",
    "    df.loc[x & ~y, \"Latitude\"] = df.loc[x & ~y, \"Latitude_x\"]\n",
    "    df.loc[x & ~y, \"Longitude\"] = df.loc[x & ~y, \"Longitude_x\"]\n",
    "    df.loc[~x & y, \"Latitude\"] = df.loc[~x & y, \"Latitude_y\"]\n",
    "    df.loc[~x & y, \"Longitude\"] = df.loc[~x & y, \"Longitude_y\"]\n",
    "    if \"Service_x\" in df.columns and \"Service_y\" in df.columns:\n",
    "        df.loc[x, \"Service\"] = df.loc[x, \"Service_x\"]\n",
    "        df.loc[~x & y, \"Service\"] = df.loc[~x & y, \"Service_y\"]\n",
    "    return df.loc[:, [c for c in df.columns if \"_\" not in c]]\n",
    "\n",
    "\n",
    "def _merge_common(dfa, dfb, dfc):\n",
    "    cols = [\"Frequency\", \"Station\"]\n",
    "    a = dfa[dfa.Station != -1].reset_index(drop=True)\n",
    "    b = dfb[dfb.Station != -1].reset_index(drop=True)\n",
    "    c = dfc[dfc.Station != -1].reset_index(drop=True)\n",
    "    common = _merge_df(a, b, cols, how=\"outer\")\n",
    "    common = _merge_df(common, c, cols, how=\"outer\")\n",
    "    common.drop_duplicates(inplace=True, keep=\"first\")\n",
    "    return common.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def _merge_new(dfa, dfb, dfc, dist=500):\n",
    "    cols = [\"Frequency\"]\n",
    "    a = (\n",
    "        dfa[dfa.Station == -1]\n",
    "        .drop(columns=[\"Service\", \"Station\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    b = (\n",
    "        dfb[dfb.Station == -1]\n",
    "        .drop(columns=[\"Service\", \"Station\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    c = (\n",
    "        dfc[dfc.Station == -1]\n",
    "        .drop(columns=[\"Service\", \"Station\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    abc = (\n",
    "        pd.merge(a, b, on=cols, how=\"outer\")\n",
    "        .merge(c, on=cols, how=\"outer\")\n",
    "        .reset_index(drop=True)\n",
    "        .drop_duplicates(keep=\"first\")\n",
    "    )\n",
    "    x = abc.Description_x.notna()\n",
    "    y = abc.Description_y.notna()\n",
    "    z = abc.Description.notna()\n",
    "    new = pd.DataFrame(columns=[\"Frequency\", \"Latitude\", \"Longitude\", \"Description\"])\n",
    "\n",
    "    for c, n in zip([(x & ~y & ~z), (~x & y & ~z), (~x & ~y & z)], [\"_x\", \"_y\", \"\"]):\n",
    "        temp = abc.loc[\n",
    "            c, [\"Frequency\", f\"Latitude{n}\", f\"Longitude{n}\", f\"Description{n}\"]\n",
    "        ]\n",
    "        temp.columns = [\"Frequency\", \"Latitude\", \"Longitude\", \"Description\"]\n",
    "        new = pd.concat([new, temp], ignore_index=True)\n",
    "\n",
    "    new = new.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "    for c, (left, right) in zip(\n",
    "        [(x & y & ~z), (x & ~y & z), (~x & y & z)],\n",
    "        [(\"_x\", \"_y\"), (\"_x\", \"\"), (\"_y\", \"\")],\n",
    "    ):\n",
    "        for row in abc[c].itertuples():\n",
    "            d = geodesic(\n",
    "                (getattr(row, f\"Latitude{left}\"), getattr(row, f\"Longitude{left}\")),\n",
    "                (getattr(row, f\"Latitude{right}\"), getattr(row, f\"Longitude{right}\")),\n",
    "            ).m\n",
    "            if d < dist:\n",
    "                f = row.Frequency\n",
    "                lat = (\n",
    "                    getattr(row, f\"Latitude{left}\") + getattr(row, f\"Latitude{right}\")\n",
    "                ) / 2\n",
    "                lon = (\n",
    "                    getattr(row, f\"Longitude{left}\") + getattr(row, f\"Longitude{right}\")\n",
    "                ) / 2\n",
    "                d = (\n",
    "                    getattr(row, f\"Description{left}\")\n",
    "                    + \" | \"\n",
    "                    + getattr(row, f\"Description{right}\")\n",
    "                )\n",
    "                new = pd.concat(\n",
    "                    [\n",
    "                        new,\n",
    "                        pd.DataFrame(\n",
    "                            [[f, lat, lon, d]],\n",
    "                            columns=[\n",
    "                                \"Frequency\",\n",
    "                                \"Latitude\",\n",
    "                                \"Longitude\",\n",
    "                                \"Description\",\n",
    "                            ],\n",
    "                        ),\n",
    "                    ],\n",
    "                    ignore_index=True,\n",
    "                )\n",
    "            else:\n",
    "                l = (\n",
    "                    abc.loc[\n",
    "                        row.Index,\n",
    "                        [\n",
    "                            \"Frequency\",\n",
    "                            f\"Latitude{left}\",\n",
    "                            f\"Longitude{left}\",\n",
    "                            f\"Description{left}\",\n",
    "                        ],\n",
    "                    ]\n",
    "                    .to_frame()\n",
    "                    .T\n",
    "                )\n",
    "                l.columns = new.columns\n",
    "                r = (\n",
    "                    abc.loc[\n",
    "                        row.Index,\n",
    "                        [\n",
    "                            \"Frequency\",\n",
    "                            f\"Latitude{right}\",\n",
    "                            f\"Longitude{right}\",\n",
    "                            f\"Description{right}\",\n",
    "                        ],\n",
    "                    ]\n",
    "                    .to_frame()\n",
    "                    .T\n",
    "                )\n",
    "                r.columns = new.columns\n",
    "                new = pd.concat([new, l, r], ignore_index=True)\n",
    "\n",
    "    new = new.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "    for row in abc[x & y & z].itertuples():\n",
    "        d1 = geodesic(\n",
    "            (getattr(row, \"Latitude_x\"), getattr(row, \"Longitude_x\")),\n",
    "            (getattr(row, \"Latitude_y\"), getattr(row, \"Longitude_y\")),\n",
    "        ).m\n",
    "        d2 = geodesic(\n",
    "            (getattr(row, \"Latitude_x\"), getattr(row, \"Longitude_x\")),\n",
    "            (getattr(row, \"Latitude\"), getattr(row, \"Longitude\")),\n",
    "        ).m\n",
    "        d3 = geodesic(\n",
    "            (getattr(row, \"Latitude_y\"), getattr(row, \"Longitude_y\")),\n",
    "            (getattr(row, \"Latitude\"), getattr(row, \"Longitude\")),\n",
    "        ).m\n",
    "        if d1 < dist and d2 < dist and d3 < dist:\n",
    "            f = row.Frequency\n",
    "            lat = (row.Latitude_x + row.Latitude_y + row.Latitude) / 3\n",
    "            lon = (row.Longitude_x + row.Longitude_y + row.Longitude) / 3\n",
    "            d = \" | \".join([row.Description_x, row.Description_y, row.Description])\n",
    "            new = pd.concat(\n",
    "                [\n",
    "                    new,\n",
    "                    pd.DataFrame(\n",
    "                        [[f, lat, lon, d]],\n",
    "                        columns=[\"Frequency\", \"Latitude\", \"Longitude\", \"Description\"],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        elif d1 < dist and d2 < dist:\n",
    "            f = row.Frequency\n",
    "            lat = (row.Latitude_x + row.Latitude_y) / 2\n",
    "            lon = (row.Longitude_x + row.Longitude_y) / 2\n",
    "            d = \" | \".join([row.Description_x, row.Description_y])\n",
    "            new = pd.concat(\n",
    "                [\n",
    "                    new,\n",
    "                    pd.DataFrame(\n",
    "                        [[f, lat, lon, d]],\n",
    "                        columns=[\"Frequency\", \"Latitude\", \"Longitude\", \"Description\"],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        elif d1 < dist and d3 < dist:\n",
    "            f = row.Frequency\n",
    "            lat = (row.Latitude_x + row.Latitude) / 2\n",
    "            lon = (row.Longitude_x + row.Longitude) / 2\n",
    "            d = \" | \".join([row.Description_x, row.Description])\n",
    "            new = pd.concat(\n",
    "                [\n",
    "                    new,\n",
    "                    pd.DataFrame(\n",
    "                        [[f, lat, lon, d]],\n",
    "                        columns=[\"Frequency\", \"Latitude\", \"Longitude\", \"Description\"],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        elif d2 < dist and d3 < dist:\n",
    "            f = row.Frequency\n",
    "            lat = (row.Latitude_y + row.Latitude) / 2\n",
    "            lon = (row.Longitude_y + row.Longitude) / 2\n",
    "            d = \" | \".join([row.Description_y, row.Description])\n",
    "            new = pd.concat(\n",
    "                [\n",
    "                    new,\n",
    "                    pd.DataFrame(\n",
    "                        [[f, lat, lon, d]],\n",
    "                        columns=[\"Frequency\", \"Latitude\", \"Longitude\", \"Description\"],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        else:\n",
    "            l = (\n",
    "                abc.loc[\n",
    "                    row.Index,\n",
    "                    [\"Frequency\", \"Latitude_x\", \"Longitude_x\", \"Description_x\"],\n",
    "                ]\n",
    "                .to_frame()\n",
    "                .T\n",
    "            )\n",
    "            l.columns = new.columns\n",
    "            r = (\n",
    "                abc.loc[\n",
    "                    row.Index,\n",
    "                    [\"Frequency\", \"Latitude_y\", \"Longitude_y\", \"Description_y\"],\n",
    "                ]\n",
    "                .to_frame()\n",
    "                .T\n",
    "            )\n",
    "            r.columns = new.columns\n",
    "            s = (\n",
    "                abc.loc[\n",
    "                    row.Index, [\"Frequency\", \"Latitude\", \"Longitude\", \"Description\"]\n",
    "                ]\n",
    "                .to_frame()\n",
    "                .T\n",
    "            )\n",
    "            s.columns = new.columns\n",
    "            new = pd.concat([new, l, r, s], ignore_index=True)\n",
    "\n",
    "    new = new.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
    "    new[\"Service\"] = -1\n",
    "    new[\"Station\"] = -1\n",
    "    return new\n",
    "\n",
    "\n",
    "def read_aero(pasta, up_icao=False, up_pmec=False, up_geo=False):\n",
    "    icao = read_icao(pasta, up_icao)\n",
    "    pmec = read_pmec(pasta, up_pmec)\n",
    "    geo = read_geo(pasta, up_geo)\n",
    "    icao[\"Description\"] = icao.Description.astype(\"string\")\n",
    "    pmec[\"Description\"] = pmec.Description.astype(\"string\")\n",
    "    geo[\"Description\"] = geo.Description.astype(\"string\")\n",
    "    common = _merge_common(icao, pmec, geo)\n",
    "    new = _merge_new(icao, pmec, geo)\n",
    "    return common, new\n",
    "\n",
    "\n",
    "def merge_aero(df, common, new):\n",
    "    \"\"\"Mescla a base da Anatel com as tabelas retiradas da Aeronáutica\"\"\"\n",
    "    common = common.loc[:, [\"Frequency\", \"Description\", \"Service\", \"Station\"]]\n",
    "    df[\"Description\"] = df.Description.astype(\"string\")\n",
    "    df = pd.merge(df, common, on=[\"Frequency\", \"Service\", \"Station\"], how=\"left\")\n",
    "    df.loc[df.Description_y.notna(), \"Description_x\"] = (\n",
    "        df.loc[df.Description_y.notna(), \"Description_x\"]\n",
    "        + \" | \"\n",
    "        + df.loc[df.Description_y.notna(), \"Description_y\"]\n",
    "    )\n",
    "    df.drop(columns=[\"Description_y\"], inplace=True)\n",
    "    df.rename(columns={\"Description_x\": \"Description\"}, inplace=True)\n",
    "    new['Class'] = '-1'\n",
    "    new['BW'] = '-1'\n",
    "    return (\n",
    "        pd.concat([df, new], ignore_index=True)\n",
    "        .sort_values(\"Frequency\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "def update_base(pasta, up_stel=False, up_radcom=False, up_mosaico=False, up_icao=False):\n",
    "    \"\"\"Wrapper que atualiza opcionalmente lê e atualiza as três bases indicadas anteriormente, as combina e salva o arquivo consolidado na pasta `pasta`\"\"\"\n",
    "    stel = read_stel(pasta, up_stel).loc[:, TELECOM]\n",
    "    radcom = read_radcom(pasta, up_radcom)\n",
    "    mosaico = read_mosaico(pasta, up_mosaico)\n",
    "    radcom[\"Num_Serviço\"] = \"231\"\n",
    "    radcom[\"Status\"] = \"RADCOM\"\n",
    "    radcom[\"Classe_Emissão\"] = \"\"\n",
    "    radcom[\"Largura_Emissão\"] = BW_MAP['231']\n",
    "    radcom[\"Classe\"] = f'{radcom.Fase.str.strip()}-{radcom.Situação.str.strip()}'\n",
    "    radcom[\"Entidade\"] = radcom.Entidade.str.rstrip().str.lstrip()\n",
    "    radcom[\"Num_Ato\"] = \"-1\"\n",
    "    radcom[\"Data_Ato\"] = \"\"\n",
    "    radcom[\"Validade_RF\"] = \"\"\n",
    "    radcom[\"Fonte\"] = \"SRD\"\n",
    "    radcom = df_optimize(radcom, exclude=[\"Frequência\"])\n",
    "    stel[\"Status\"] = \"L\"\n",
    "    stel[\"Num_Ato\"] = \"-1\"\n",
    "    stel[\"Data_Ato\"] = \"\"\n",
    "    stel[\"Entidade\"] = stel.Entidade.str.rstrip().str.lstrip()\n",
    "    stel[\"Fonte\"] = \"STEL\"\n",
    "    stel = df_optimize(stel, exclude=[\"Frequência\"])\n",
    "    mosaico[\"Fonte\"] = \"MOS\"\n",
    "    mosaico[\"Classe_Emissão\"] = \"\"\n",
    "    mosaico[\"Largura_Emissão\"] = mosaico.Num_Serviço.map(BW_MAP)\n",
    "    mosaico = mosaico.loc[:, RADIODIFUSAO]\n",
    "    mosaico = df_optimize(mosaico, exclude=[\"Frequência\"])\n",
    "    rd = (\n",
    "        pd.concat([mosaico, radcom, stel])\n",
    "        .sort_values(\"Frequência\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    rd[\"Num_Serviço\"] = rd.Num_Serviço.astype(\"int\")\n",
    "    rd = df_optimize(rd, exclude=[\"Frequência\"])\n",
    "    rd = rd.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
    "    rd['BW(kHz)'] = rd.Largura_Emissão.apply(parse_bw)\n",
    "    console.print(\":trophy: [green]Base Consolidada. Salvando os arquivos...\")\n",
    "    try:\n",
    "        rd.to_feather(f\"{pasta}/base.fth\")\n",
    "    except ArrowInvalid:\n",
    "        Path(f\"{pasta}/base.fth\").unlink()\n",
    "        with pd.ExcelWriter(f\"{pasta}/base.xlsx\") as workbook:\n",
    "            rd.to_excel(workbook, sheet_name=\"Sheet1\", engine=\"openpyxl\", index=False)\n",
    "    return rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de Leitura das diversas bases\n",
    "A presente biblioteca utiliza três bases de dados: \n",
    "* `STEL` - Serviços Privados de Telecomunicações\n",
    "* `RADCOM` - Serviço de Radiodifusão comunitária\n",
    "* `MOSAICO` - Demais serviços de Radiodifusão e paulatinamente também adicionado serviços privados\n",
    "\n",
    "As funções a seguir são para leitura dos arquivos locais baixados e processados dessas bases, opcionalmente esses arquivos podem ser atualizados antes de serem lidos passando o argumento `update = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_stel(pasta, update=False):\n",
    "    \"\"\"Lê o banco de dados salvo localmente do STEL. Opcionalmente o atualiza pelo Banco de Dados ANATELBDRO01 caso `update = True` ou não exista o arquivo local\"\"\"\n",
    "    if update:\n",
    "        update_stel(pasta)\n",
    "    file = Path(f\"{pasta}/stel.fth\")\n",
    "    try:\n",
    "        stel = pd.read_feather(file)\n",
    "    except (ArrowInvalid, FileNotFoundError):\n",
    "        file = Path(f\"{pasta}/stel.xlsx\")\n",
    "        try:\n",
    "            stel = pd.read_excel(file, engine=\"openpyxl\")\n",
    "        except FileNotFoundError:\n",
    "            read_stel(pasta, True)\n",
    "\n",
    "    return stel\n",
    "\n",
    "\n",
    "def read_radcom(pasta, update=False):\n",
    "    \"\"\"Lê o banco de dados salvo localmente de RADCOM. Opcionalmente o atualiza pelo Banco de Dados ANATELBDRO01 caso `update = True` ou não exista o arquivo local\"\"\"\n",
    "    if update:\n",
    "        update_radcom(pasta)\n",
    "    file = Path(f\"{pasta}/radcom.fth\")\n",
    "    try:\n",
    "        radcom = pd.read_feather(file)\n",
    "    except (ArrowInvalid, FileNotFoundError):\n",
    "        file = Path(f\"{pasta}/radcom.xlsx\")\n",
    "        try:\n",
    "            radcom = pd.read_excel(file, engine=\"openpyxl\")\n",
    "        except FileNotFoundError:\n",
    "            read_radcom(pasta, True)\n",
    "    return radcom\n",
    "\n",
    "\n",
    "def read_mosaico(pasta, update=False):\n",
    "    \"\"\"Lê o banco de dados salvo localmente do MOSAICO. Opcionalmente o atualiza antes da leitura baixando os diversos arquivos disponíveis na interface web pública\"\"\"\n",
    "    if update:\n",
    "        update_mosaico(pasta)\n",
    "    file = Path(f\"{pasta}/mosaico.fth\")\n",
    "    try:\n",
    "        df = pd.read_feather(file)\n",
    "    except (ArrowInvalid, FileNotFoundError):\n",
    "        file = Path(f\"{pasta}/mosaico.xlsx\")\n",
    "        try:\n",
    "            df = pd.read_excel(file)\n",
    "        except FileNotFoundError:\n",
    "            return read_mosaico(pasta, update=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_icao(pasta, update=False):\n",
    "    \"\"\"Lê a base de dados do Frequency Finder e Canalização VOR/ILS/DME\"\"\"\n",
    "    if update:\n",
    "        # TODO: atualizar a base de dados do Frequency Finder e Canalização VOR/ILS/DME\n",
    "        # update_icao(pasta)\n",
    "        raise NotImplementedError(\n",
    "            \"Atualizar da base de dados do Frequency Finder e Canalização VOR/ILS/DME não implementado\"\n",
    "        )\n",
    "    file = Path(f\"{pasta}/IcaoDB.fth\")\n",
    "    try:\n",
    "        icao = pd.read_feather(file)\n",
    "    except (ArrowInvalid, FileNotFoundError):\n",
    "        file = Path(f\"{pasta}/IcaoDB.xlsx\")\n",
    "        try:\n",
    "            icao = pd.read_excel(file, sheet_name=\"DataBase\", engine=\"openpyxl\")\n",
    "        except FileNotFoundError:\n",
    "            return read_icao(pasta, update=True)\n",
    "    return df_optimize(icao, exclude=[\"Frequency\"])\n",
    "\n",
    "\n",
    "def read_pmec(pasta, update=False):\n",
    "    \"\"\"Fontes da informação: AISWEB, REDEMET, Ofício nº 2/SSARP/14410 e Canalização VOR/ILS/DME.\"\"\"\n",
    "    if update:\n",
    "        # TODO: Atualizar a base de dados do AISWEB, REDEMET, Ofício nº 2/SSARP/14410 e Canalização VOR/ILS/DME\n",
    "        # update_pmec(pasta)\n",
    "        raise NotImplementedError(\n",
    "            \"Atualizar da base de dados do Frequency Finder e Canalização VOR/ILS/DME não implementado\"\n",
    "        )\n",
    "    file = Path(f\"{pasta}/PmecDB.fth\")\n",
    "    try:\n",
    "        df = pd.read_feather(file)\n",
    "    except (ArrowInvalid, FileNotFoundError):\n",
    "        file = Path(f\"{pasta}/PmecDB.xlsx\")\n",
    "        try:\n",
    "            df = pd.read_excel(file, sheet_name=\"DataBase\", engine=\"openpyxl\")\n",
    "        except FileNotFoundError:\n",
    "            return read_icao(pasta, update=True)\n",
    "    return df_optimize(df, exclude=[\"Frequency\"])\n",
    "\n",
    "\n",
    "def read_geo(pasta, update=False):\n",
    "    \"\"\"Fontes da informação: AISWEB, REDEMET, Ofício nº 2/SSARP/14410 e Canalização VOR/ILS/DME.\"\"\"\n",
    "    if update:\n",
    "        # TODO: Atualizar a base de dados do GEOAISWEB\n",
    "        # update_geo(pasta)\n",
    "        raise NotImplementedError(\n",
    "            \"Atualizar da base de dados do Frequency Finder e Canalização VOR/ILS/DME não implementado\"\n",
    "        )\n",
    "    file = Path(f\"{pasta}/GeoAiswebDB.fth\")\n",
    "    try:\n",
    "        df = pd.read_feather(file)\n",
    "    except (ArrowInvalid, FileNotFoundError):\n",
    "        file = Path(f\"{pasta}/GeoAiswebDB.xlsx\")\n",
    "        try:\n",
    "            df = pd.read_excel(file, sheet_name=\"DataBase\", engine=\"openpyxl\")\n",
    "        except FileNotFoundError:\n",
    "            return read_icao(pasta, update=True)\n",
    "    return df_optimize(df, exclude=[\"Frequency\"])\n",
    "\n",
    "\n",
    "def read_base(pasta, up_stel=False, up_radcom=False, up_mosaico=False, up_icao=False):\n",
    "    \"\"\"Wrapper que combina a chamada das três funções de leitura do banco e opcionalmente é possível atualizá-las antes da leitura\"\"\"\n",
    "    if any([up_stel, up_radcom, up_mosaico, up_icao]):\n",
    "        update_base(pasta, up_stel, up_radcom, up_mosaico, up_icao)\n",
    "    file = Path(f\"{pasta}/base.fth\")\n",
    "    try:\n",
    "        df = pd.read_feather(file)\n",
    "    except (ArrowInvalid, FileNotFoundError):\n",
    "        file = Path(f\"{pasta}/base.xlsx\")\n",
    "        try:\n",
    "            df = pd.read_excel(file, engine=\"openpyxl\")\n",
    "        except FileNotFoundError:\n",
    "            df = update_base(pasta, True, True, True)\n",
    "    return df_optimize(df, exclude=[\"Frequência\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted constants.ipynb.\n",
      "Converted filter.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted queries.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anateldb]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
