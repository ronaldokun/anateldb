{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp query\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Queries\n",
    "\n",
    "> Este m√≥dulo executa as queries sql / MongoDB necess√°rias para baixar os dados do STEL, RADCOM e MOSAICO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import requests\n",
    "import re\n",
    "from decimal import *\n",
    "from typing import *\n",
    "from gazpacho import Soup\n",
    "from rich.progress import track\n",
    "from pathlib import Path\n",
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "import pandas_read_xml as pdx\n",
    "import pyodbc\n",
    "import collections\n",
    "from fastcore.utils import listify\n",
    "from fastcore.foundation import L\n",
    "from fastcore.test import *\n",
    "from anateldb.constants import *\n",
    "from pyarrow import ArrowInvalid\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "getcontext().prec = 5\n",
    "\n",
    "BW = {'H': 0.001, 'K': 1, 'M': 1000, 'G': 1000000}\n",
    "BW_pattern = re.compile(\"^(\\d{1,3})([HKMG])(\\d{0,2})$\")\n",
    "BW_MAP = {'230': '256K', '231': '256K', '205': '10K0', '800': '6M00', '801': '5M70', '248': '6M00', '247': '5M70', '167': '6M00'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pasta = Path('c:/Users/rsilva/AnatelDatabase')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Otimiza√ß√£o dos Tipos de dados\n",
    "A serem criados dataframes, normalmente a tipo de data √© aquele com maior resolu√ß√£o poss√≠vel, nem sempre isso √© necess√°rio, os arquivos de espectro mesmo possuem somente uma casa decimal, portanto um `float16` j√° √© suficiente para armazen√°-los. As fun√ß√µes a seguir fazem essa otimiza√ß√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code below borrowed from https://medium.com/bigdatarepublic/advanced-pandas-optimize-speed-and-memory-a654b53be6c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def optimize_floats(df: pd.DataFrame, exclude=None) -> pd.DataFrame:\n",
    "    floats = df.select_dtypes(include=[\"float64\"]).columns.tolist()\n",
    "    floats = [c for c in floats if c not in listify(exclude)]\n",
    "    df[floats] = df[floats].apply(pd.to_numeric, downcast=\"float\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def optimize_ints(df: pd.DataFrame, exclude=None) -> pd.DataFrame:\n",
    "    ints = df.select_dtypes(include=[\"int64\"]).columns.tolist()\n",
    "    ints = [c for c in ints if c not in listify(exclude)]\n",
    "    df[ints] = df[ints].apply(pd.to_numeric, downcast=\"integer\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def optimize_objects(\n",
    "    df: pd.DataFrame, datetime_features: List[str], exclude=None\n",
    ") -> pd.DataFrame:\n",
    "    for col in df.select_dtypes(include=[\"object\"]).columns.tolist():\n",
    "        if col not in datetime_features:\n",
    "            if col in listify(exclude):\n",
    "                continue\n",
    "            num_unique_values = len(df[col].unique())\n",
    "            num_total_values = len(df[col])\n",
    "            if float(num_unique_values) / num_total_values < 0.5:\n",
    "                dtype = \"category\"\n",
    "            else:\n",
    "                dtype = \"string\"\n",
    "            df[col] = df[col].astype(dtype)\n",
    "        else:\n",
    "            df[col] = pd.to_datetime(df[col]).dt.date\n",
    "    return df\n",
    "\n",
    "\n",
    "def df_optimize(df: pd.DataFrame, datetime_features: List[str] = None, exclude=None):\n",
    "    if datetime_features is None:\n",
    "        datetime_features = []\n",
    "    return optimize_floats(\n",
    "        optimize_ints(optimize_objects(df, datetime_features, exclude), exclude),\n",
    "        exclude,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conex√£o com o banco de dados\n",
    "A fun√ß√£o a seguir √© um `wrapper` simples que utiliza o `pyodbc` para se conectar ao banco de dados base da Anatel e retorna o objeto da conex√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def connect_db():\n",
    "    \"\"\"Conecta ao Banco ANATELBDRO01 e retorna o 'cursor' (iterador) do Banco pronto para fazer itera√ß√µes\"\"\"\n",
    "    return pyodbc.connect(\n",
    "        \"Driver={ODBC Driver 17 for SQL Server};\"\n",
    "        \"Server=ANATELBDRO01;\"\n",
    "        \"Database=SITARWEB;\"\n",
    "        \"Trusted_Connection=yes;\"\n",
    "        \"MultipleActiveResultSets=True;\",\n",
    "        timeout=TIMEOUT,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#slow\n",
    "def test_connection():\n",
    "    conn = connect_db()\n",
    "    cursor = conn.cursor()\n",
    "    for query in (RADCOM, STEL):\n",
    "        cursor.execute(query)\n",
    "        test_eq(type(cursor.fetchone()), pyodbc.Row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fun√ß√µes auxiliares de Formata√ß√£o\n",
    "As fun√ß√µes a seguir s√£o utilizadas para formatar diversos objetos intermedi√°rios utilizados nas fun√ß√µes de leitura e atualiza√ß√£o da base de dados e n√£o s√£o chamadas diretamente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def row2dict(row):\n",
    "    \"\"\"Receives a json row and return the dictionary from it\"\"\"\n",
    "    return dict(row.items())\n",
    "\n",
    "\n",
    "def dict2cols(df, reject=()):\n",
    "    \"\"\"Recebe um dataframe com dicion√°rios nas c√©lulas e extrai os dicion√°rios como colunas\n",
    "    Opcionalmente ignora e exclue as colunas em reject\n",
    "    \"\"\"\n",
    "    for column in df.columns:\n",
    "        if column in reject:\n",
    "            df.drop(column, axis=1, inplace=True)\n",
    "            continue\n",
    "        if type(df[column].iloc[0]) == collections.OrderedDict:\n",
    "            try:\n",
    "                new_df = pd.DataFrame(df[column].apply(row2dict).tolist())\n",
    "                df = pd.concat([df, new_df], axis=1)\n",
    "                df.drop(column, axis=1, inplace=True)\n",
    "            except AttributeError:\n",
    "                continue\n",
    "    return df\n",
    "\n",
    "\n",
    "def parse_plano_basico(row, cols=COL_PB):\n",
    "    \"\"\"Receives a json row and filter the column in `cols`\"\"\"\n",
    "    return {k: row[k] for k in cols}\n",
    "\n",
    "\n",
    "def scrape_dataframe(id_list):\n",
    "    df = pd.DataFrame()\n",
    "    for id_ in track(id_list, description=\"Baixando informa√ß√µes complementares da Web\"):\n",
    "        html = requests.get(ESTACAO.format(id_))\n",
    "        df = df.append(pd.read_html(Soup(html.text).find(\"table\").html)[0])\n",
    "\n",
    "    df.rename(\n",
    "        columns={\"NumFistel\": \"Fistel\", \"Num Servi√ßo\": \"Num_Servi√ßo\"}, inplace=True\n",
    "    )\n",
    "    return df[\n",
    "        [\n",
    "            \"Status\",\n",
    "            \"Entidade\",\n",
    "            \"Fistel\",\n",
    "            \"Frequ√™ncia\",\n",
    "            \"Classe\",\n",
    "            \"Num_Servi√ßo\",\n",
    "            \"Munic√≠pio\",\n",
    "            \"UF\",\n",
    "        ]\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def input_coordenates(df, pasta):\n",
    "    \"\"\"Input the NA's in Coordinates with those of the cities\"\"\"\n",
    "    municipios = Path(f\"{pasta}/munic√≠pios.fth\")\n",
    "    if not municipios.exists():\n",
    "        municipios = Path(f\"{pasta}/munic√≠pios.xlsx\")\n",
    "        if not municipios.exists():\n",
    "            raise FileNotFoundError(\n",
    "                f\"√â necessario a tabela de munic√≠pios munic√≠pios.fth | munic√≠pios.xlsx na pasta {pasta}\"\n",
    "            )\n",
    "        m = pd.read_excel(municipios, engine=\"openpyxl\")\n",
    "    else:\n",
    "        m = pd.read_feather(municipios)\n",
    "    m.loc[\n",
    "        m.Munic√≠pio == \"Sant'Ana do Livramento\", \"Munic√≠pio\"\n",
    "    ] = \"Santana do Livramento\"\n",
    "    m[\"Munic√≠pio\"] = m.Munic√≠pio.apply(unidecode).str.lower().str.replace(\"'\", \" \")\n",
    "    m[\"UF\"] = m.UF.str.lower()\n",
    "    df[\"Coordenadas_do_Munic√≠pio\"] = False\n",
    "    df[\"Latitude\"] = df.Latitude.str.replace(\",\", \".\")\n",
    "    df[\"Longitude\"] = df.Longitude.str.replace(\",\", \".\")\n",
    "    df.loc[df[\"Munic√≠pio\"] == \"Poxor√©o\", \"Munic√≠pio\"] = \"Poxor√©u\"\n",
    "    df.loc[df[\"Munic√≠pio\"] == \"Couto de Magalh√£es\", \"Munic√≠pio\"] = \"Couto Magalh√£es\"\n",
    "    df[\"Munic√≠pio\"] = df.Munic√≠pio.astype(\"string\")\n",
    "    criteria = (\n",
    "        (df.Latitude == \"\")\n",
    "        | (df.Latitude.isna())\n",
    "        | (df.Longitude == \"\")\n",
    "        | (df.Longitude.isna())\n",
    "    ) & df.Munic√≠pio.isna()\n",
    "    df = df[~criteria]\n",
    "    for row in df[\n",
    "        (\n",
    "            (df.Latitude == \"\")\n",
    "            | (df.Latitude.isna())\n",
    "            | (df.Longitude == \"\")\n",
    "            | (df.Longitude.isna())\n",
    "        )\n",
    "    ].itertuples():\n",
    "        try:\n",
    "            left = unidecode(row.Munic√≠pio).lower()\n",
    "            m_coord = (\n",
    "                m.loc[\n",
    "                    (m.Munic√≠pio == left) & (m.UF == row.UF.lower()),\n",
    "                    [\"Latitude\", \"Longitude\"],\n",
    "                ]\n",
    "                .values.flatten()\n",
    "                .tolist()\n",
    "            )\n",
    "            df.loc[row.Index, \"Latitude\"] = m_coord[0]\n",
    "            df.loc[row.Index, \"Longitude\"] = m_coord[1]\n",
    "            df.loc[row.Index, \"Coordenadas_do_Munic√≠pio\"] = True\n",
    "        except ValueError:\n",
    "            print(left, row.UF, m_coord)\n",
    "            continue\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_merge(pasta, df):\n",
    "    df = df.copy()\n",
    "    COLS = [c for c in df.columns if \"_x\" in c]\n",
    "    for col in COLS:\n",
    "        col_x = col\n",
    "        col_y = col.split(\"_\")[0] + \"_y\"\n",
    "        if df[col_x].count() > df[col_y].count():\n",
    "            a, b = col_x, col_y\n",
    "        else:\n",
    "            a, b = col_y, col_x\n",
    "\n",
    "        df.loc[df[a].isna(), a] = df.loc[df[a].isna(), b]\n",
    "        df.drop(b, axis=1, inplace=True)\n",
    "        df.rename({a: a[:-2]}, axis=1, inplace=True)\n",
    "\n",
    "    df.loc[df.Latitude_Transmissor == \"\", \"Latitude_Transmissor\"] = df.loc[\n",
    "        df.Latitude_Transmissor == \"\", \"Latitude_Esta√ß√£o\"\n",
    "    ]\n",
    "    df.loc[df.Longitude_Transmissor == \"\", \"Longitude_Transmissor\"] = df.loc[\n",
    "        df.Longitude_Transmissor == \"\", \"Longitude_Esta√ß√£o\"\n",
    "    ]\n",
    "    df.loc[df.Latitude_Transmissor.isna(), \"Latitude_Transmissor\"] = df.loc[\n",
    "        df.Latitude_Transmissor.isna(), \"Latitude_Esta√ß√£o\"\n",
    "    ]\n",
    "    df.loc[df.Longitude_Transmissor.isna(), \"Longitude_Transmissor\"] = df.loc[\n",
    "        df.Longitude_Transmissor.isna(), \"Longitude_Esta√ß√£o\"\n",
    "    ]\n",
    "    df.drop([\"Latitude_Esta√ß√£o\", \"Longitude_Esta√ß√£o\"], axis=1, inplace=True)\n",
    "    df.rename(\n",
    "        columns={\n",
    "            \"Latitude_Transmissor\": \"Latitude\",\n",
    "            \"Longitude_Transmissor\": \"Longitude\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    df = input_coordenates(df, pasta)\n",
    "\n",
    "    df[\"Frequ√™ncia\"] = df.Frequ√™ncia.str.replace(\",\", \".\")\n",
    "\n",
    "    if freq_nans := df[df.Frequ√™ncia.isna()].Id.tolist():\n",
    "        complement_df = scrape_dataframe(freq_nans)\n",
    "        df.loc[\n",
    "            df.Frequ√™ncia.isna(),\n",
    "            [\n",
    "                \"Status\",\n",
    "                \"Entidade\",\n",
    "                \"Fistel\",\n",
    "                \"Frequ√™ncia\",\n",
    "                \"Classe\",\n",
    "                \"Num_Servi√ßo\",\n",
    "                \"Munic√≠pio\",\n",
    "                \"UF\",\n",
    "            ],\n",
    "        ] = complement_df.values\n",
    "\n",
    "    for r in df[(df.Entidade.isna()) | (df.Entidade == \"\")].itertuples():\n",
    "        df.loc[r.Index, \"Entidade\"] = ENTIDADES.get(r.Fistel, \"\")\n",
    "\n",
    "    df.loc[df[\"N√∫mero_da_Esta√ß√£o\"] == \"\", \"N√∫mero_da_Esta√ß√£o\"] = -1\n",
    "    df[\"Latitude\"] = df[\"Latitude\"].astype(\"float\")\n",
    "    df[\"Longitude\"] = df[\"Longitude\"].astype(\"float\")\n",
    "    df[\"Frequ√™ncia\"] = df.Frequ√™ncia.astype(\"float\")\n",
    "    df.loc[df.Servi√ßo == \"OM\", \"Frequ√™ncia\"] = df.loc[\n",
    "        df.Servi√ßo == \"OM\", \"Frequ√™ncia\"\n",
    "    ].apply(lambda x: Decimal(x) / Decimal(1000))\n",
    "    df[\"Frequ√™ncia\"] = df.Frequ√™ncia.astype(\"float\")\n",
    "    df[\"Validade_RF\"] = df.Validade_RF.astype(\"string\").str.slice(0, 10)\n",
    "    df.loc[df[\"Num_Ato\"] == \"\", \"Num_Ato\"] = -1\n",
    "    df[\"Num_Ato\"] = df.Num_Ato.astype(\"string\")\n",
    "    df[\"Num_Servi√ßo\"] = df.Num_Servi√ßo.astype(\"category\")\n",
    "    return df_optimize(df, exclude=[\"Frequ√™ncia\"])\n",
    "\n",
    "def parse_bw(bw):\n",
    "    if match := re.match(BW_pattern, bw):\n",
    "        multiplier = BW[match.group(2)]\n",
    "        if mantissa := match.group(3):\n",
    "            number = float(f'{match.group(1)}.{mantissa}')\n",
    "        else:\n",
    "            number = float(match.group(1))\n",
    "        return multiplier * number\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fun√ß√µes de Leitura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def read_esta√ß√µes(path):\n",
    "    def extrair_ato(row):\n",
    "        if not isinstance(row, str):\n",
    "            row = listify(row)[::-1]\n",
    "            for d in row:\n",
    "                if not isinstance(d, dict):\n",
    "                    continue\n",
    "                if (d.get(\"@TipoDocumento\") == \"Ato\") and (\n",
    "                    d.get(\"@Razao\") == \"Autoriza o Uso de Radiofrequ√™ncia\"\n",
    "                ):\n",
    "                    return d[\"@NumDocumento\"], d[\"@DataDOU\"][:10]\n",
    "            return \"\", \"\"\n",
    "        return \"\", \"\"\n",
    "\n",
    "    es = pdx.read_xml(path, [\"estacao_rd\"])\n",
    "    dfs = []\n",
    "    for i in range(es.shape[0]):\n",
    "        df = pd.DataFrame(es[\"row\"][i]).replace(\"\", pd.NA)\n",
    "        df = dict2cols(df)\n",
    "        df.columns = [unidecode(c).lower().replace(\"@\", \"\") for c in df.columns]\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs)\n",
    "    df = df[df.state.str.contains(\"-C1$|-C2$|-C3$|-C4$|-C7|-C98$\")].reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    docs = L(df.historico_documentos.apply(extrair_ato).tolist())\n",
    "    df = df.loc[:, COL_ESTACOES]\n",
    "    df[\"Num_Ato\"] = docs.itemgot(0).map(str)\n",
    "    df[\"Data_Ato\"] = docs.itemgot(1).map(str)\n",
    "    df.columns = NEW_ESTACOES\n",
    "    df[\"Validade_RF\"] = df.Validade_RF.astype(\"string\").str.slice(0, 10)\n",
    "    df[\"Data_Ato\"] = df.Data_Ato.str.slice(0, 10)\n",
    "    for c in df.columns:\n",
    "        df.loc[df[c] == \"\", c] = pd.NA\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_plano_basico(path):\n",
    "    pb = pdx.read_xml(path, [\"plano_basico\"])\n",
    "    dfs = []\n",
    "    for i in range(pb.shape[0]):\n",
    "        df = pd.DataFrame(pb[\"row\"][i]).replace(\"\", pd.NA)\n",
    "        df = dict2cols(df)\n",
    "        df.columns = [unidecode(c).lower().replace(\"@\", \"\") for c in df.columns]\n",
    "        dfs.append(df)\n",
    "    df = pd.concat(dfs)\n",
    "    df = df.loc[df.pais == \"BRA\", COL_PB].reset_index(drop=True)\n",
    "    for c in df.columns:\n",
    "        df.loc[df[c] == \"\", c] = pd.NA\n",
    "    df.columns = NEW_PB\n",
    "    df.sort_values([\"Id\", \"Canal\"], inplace=True)\n",
    "    ENTIDADES.update(\n",
    "        {r.Fistel: r.Entidade for r in df.itertuples() if str(r.Entidade) == \"<NA>\"}\n",
    "    )\n",
    "    df = df.groupby(\"Id\", as_index=False).first()  # remove duplicated with NaNs\n",
    "    df.dropna(subset=[\"Status\"], inplace=True)\n",
    "    df = df[df.Status.str.contains(\"-C1$|-C2$|-C3$|-C4$|-C7|-C98$\")].reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Atualiza√ß√£o das bases de dados\n",
    "As bases de dados s√£o atualizadas atr√°ves das fun√ß√µes a seguir, o √∫nico argumento passado em todas elas √© a pasta na qual os arquivos locais processados ser√£o salvos, os nomes dos arquivos s√£o padronizados e n√£o podem ser editados para que as fun√ß√µes de leitura e processamento recebam somente a pasta na qual esses arquivos foram salvos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def update_radcom(pasta):\n",
    "    \"\"\"Atualiza a tabela local retornada pela query `RADCOM`\"\"\"\n",
    "    with console.status(\n",
    "        \"[cyan]Lendo o Banco de Dados de Radcom...\", spinner=\"earth\"\n",
    "    ) as status:\n",
    "        try:\n",
    "            conn = connect_db()\n",
    "            df = pd.read_sql_query(RADCOM, conn)\n",
    "            df[\"Unidade\"] = \"MHz\"\n",
    "            df = df_optimize(df, exclude=[\"Frequ√™ncia\"])\n",
    "            try:\n",
    "                df.to_feather(f\"{pasta}/radcom.fth\")\n",
    "            except ArrowInvalid:\n",
    "                Path(f\"{pasta}/radcom.fth\").unlink()\n",
    "                df.to_excel(f\"{pasta}/radcom.xlsx\", engine=\"openpyxl\", index=False)\n",
    "        except pyodbc.OperationalError:\n",
    "            status.console.log(\n",
    "                \"N√£o foi poss√≠vel abrir uma conex√£o com o SQL Server. Esta conex√£o somente funciona da rede cabeada!\"\n",
    "            )\n",
    "\n",
    "\n",
    "def update_stel(pasta):\n",
    "    \"\"\"Atualiza a tabela local retornada pela query `STEL`\"\"\"\n",
    "    with console.status(\n",
    "        \"[magenta]Lendo o Banco de Dados do STEL. Processo Lento, aguarde...\",\n",
    "        spinner=\"moon\",\n",
    "    ) as status:\n",
    "        try:\n",
    "            conn = connect_db()\n",
    "            df = pd.read_sql_query(STEL, conn)\n",
    "            df[\"Validade_RF\"] = df.Validade_RF.astype(\"str\").str.slice(0, 10)\n",
    "            df[\"Num_Servi√ßo\"] = df.Num_Servi√ßo.astype(\"category\")\n",
    "            df.loc[df.Unidade == \"kHz\", \"Frequ√™ncia\"] = df.loc[\n",
    "                df.Unidade == \"kHz\", \"Frequ√™ncia\"\n",
    "            ].apply(lambda x: Decimal(x) / Decimal(1000))\n",
    "            df.loc[df.Unidade == \"GHz\", \"Frequ√™ncia\"] = df.loc[\n",
    "                df.Unidade == \"GHz\", \"Frequ√™ncia\"\n",
    "            ].apply(lambda x: Decimal(x) * Decimal(1000))\n",
    "            df[\"Frequ√™ncia\"] = df.Frequ√™ncia.astype(\"float\")\n",
    "            df.loc[df.Unidade == \"kHz\", \"Unidade\"] = \"MHz\"\n",
    "            df = df_optimize(df, exclude=[\"Frequ√™ncia\"])\n",
    "            try:\n",
    "                df.to_feather(f\"{pasta}/stel.fth\")\n",
    "            except ArrowInvalid:\n",
    "                Path(f\"{pasta}/stel.fth\").unlink()\n",
    "                df.to_excel(f\"{pasta}/stel.xlsx\", engine=\"openpyxl\", index=False)\n",
    "        except pyodbc.OperationalError:\n",
    "            status.console.log(\n",
    "                \"N√£o foi poss√≠vel abrir uma conex√£o com o SQL Server. Esta conex√£o somente funciona da rede cabeada!\"\n",
    "            )\n",
    "\n",
    "\n",
    "def update_mosaico(pasta):\n",
    "    \"\"\"Atualiza a tabela local do Mosaico. √â baixado e processado arquivos xml zipados da p√°gina p√∫blica do Spectrum E\"\"\"\n",
    "    with console.status(\n",
    "        \"[blue]Baixando as Esta√ß√µes do Mosaico...\", spinner=\"shark\"\n",
    "    ) as status:\n",
    "        file = requests.get(ESTACOES)\n",
    "        with open(f\"{pasta}/esta√ß√µes.zip\", \"wb\") as esta√ß√µes:\n",
    "            esta√ß√µes.write(file.content)\n",
    "    with console.status(\n",
    "        \"[blue]Baixando o Plano B√°sico das Esta√ß√µes...\", spinner=\"weather\"\n",
    "    ) as status:\n",
    "        file = requests.get(PLANO_BASICO)\n",
    "        with open(f\"{pasta}/Canais.zip\", \"wb\") as plano_basico:\n",
    "            plano_basico.write(file.content)\n",
    "    console.print(\":package: [blue]Consolidando as bases de dados...\")\n",
    "    esta√ß√µes = read_esta√ß√µes(f\"{pasta}/esta√ß√µes.zip\")\n",
    "    plano_basico = read_plano_basico(f\"{pasta}/Canais.zip\")\n",
    "    df = esta√ß√µes.merge(plano_basico, on=\"Id\", how=\"left\")\n",
    "    df[\"N√∫mero_da_Esta√ß√£o\"] = df[\"N√∫mero_da_Esta√ß√£o\"].fillna(-1)\n",
    "    df[\"N√∫mero_da_Esta√ß√£o\"] = df[\"N√∫mero_da_Esta√ß√£o\"].astype(\"int\")\n",
    "    df = clean_merge(pasta, df)\n",
    "    try:\n",
    "        df.reset_index(drop=True).to_feather(f\"{pasta}/mosaico.fth\")\n",
    "    except ArrowInvalid:\n",
    "        Path(f\"{pasta}/mosaico.fth\").unlink()\n",
    "        with pd.ExcelWriter(f\"{pasta}/mosaico.xlsx\") as workbook:\n",
    "            df.reset_index(drop=True).to_excel(\n",
    "                workbook, sheet_name=\"Sheet1\", engine=\"openpyxl\", index=False\n",
    "            )\n",
    "    console.print(\"Kb√¥ :vampire:\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def _merge_df(df1, df2, on, how=\"left\"):\n",
    "    \"\"\"Merge two dataframes with the same columns and records\"\"\"\n",
    "    df = pd.merge(df1, df2, on=on, how=how)\n",
    "    x = df.Description_x.notna()\n",
    "    y = df.Description_y.notna()\n",
    "    df.loc[x & y, \"Description\"] = (\n",
    "        df.loc[x & y, \"Description_x\"] + \" | \" + df.loc[x & y, \"Description_y\"]\n",
    "    )\n",
    "    df.loc[x & ~y, \"Description\"] = df.loc[x & ~y, \"Description_x\"]\n",
    "    df.loc[~x & y, \"Description\"] = df.loc[~x & y, \"Description_y\"]\n",
    "    df.loc[x & y, \"Latitude\"] = (\n",
    "        df.loc[x & y, \"Latitude_x\"] + df.loc[x & y, \"Latitude_y\"]\n",
    "    ) / 2\n",
    "    df.loc[x & y, \"Longitude\"] = (\n",
    "        df.loc[x & y, \"Longitude_x\"] + df.loc[x & y, \"Longitude_y\"]\n",
    "    ) / 2\n",
    "    df.loc[x & ~y, \"Latitude\"] = df.loc[x & ~y, \"Latitude_x\"]\n",
    "    df.loc[x & ~y, \"Longitude\"] = df.loc[x & ~y, \"Longitude_x\"]\n",
    "    df.loc[~x & y, \"Latitude\"] = df.loc[~x & y, \"Latitude_y\"]\n",
    "    df.loc[~x & y, \"Longitude\"] = df.loc[~x & y, \"Longitude_y\"]\n",
    "    if \"Service_x\" in df.columns and \"Service_y\" in df.columns:\n",
    "        df.loc[x, \"Service\"] = df.loc[x, \"Service_x\"]\n",
    "        df.loc[~x & y, \"Service\"] = df.loc[~x & y, \"Service_y\"]\n",
    "    return df.loc[:, [c for c in df.columns if \"_\" not in c]]\n",
    "\n",
    "\n",
    "def _merge_common(dfa, dfb, dfc):\n",
    "    cols = [\"Frequency\", \"Station\"]\n",
    "    a = dfa[dfa.Station != -1].reset_index(drop=True)\n",
    "    b = dfb[dfb.Station != -1].reset_index(drop=True)\n",
    "    c = dfc[dfc.Station != -1].reset_index(drop=True)\n",
    "    common = _merge_df(a, b, cols, how=\"outer\")\n",
    "    common = _merge_df(common, c, cols, how=\"outer\")\n",
    "    common.drop_duplicates(inplace=True, keep=\"first\")\n",
    "    return common.reset_index(drop=True)\n",
    "\n",
    "\n",
    "def _merge_new(dfa, dfb, dfc, dist=500):\n",
    "    cols = [\"Frequency\"]\n",
    "    a = (\n",
    "        dfa[dfa.Station == -1]\n",
    "        .drop(columns=[\"Service\", \"Station\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    b = (\n",
    "        dfb[dfb.Station == -1]\n",
    "        .drop(columns=[\"Service\", \"Station\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    c = (\n",
    "        dfc[dfc.Station == -1]\n",
    "        .drop(columns=[\"Service\", \"Station\"])\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    abc = (\n",
    "        pd.merge(a, b, on=cols, how=\"outer\")\n",
    "        .merge(c, on=cols, how=\"outer\")\n",
    "        .reset_index(drop=True)\n",
    "        .drop_duplicates(keep=\"first\")\n",
    "    )\n",
    "    x = abc.Description_x.notna()\n",
    "    y = abc.Description_y.notna()\n",
    "    z = abc.Description.notna()\n",
    "    new = pd.DataFrame(columns=[\"Frequency\", \"Latitude\", \"Longitude\", \"Description\"])\n",
    "\n",
    "    for c, n in zip([(x & ~y & ~z), (~x & y & ~z), (~x & ~y & z)], [\"_x\", \"_y\", \"\"]):\n",
    "        temp = abc.loc[\n",
    "            c, [\"Frequency\", f\"Latitude{n}\", f\"Longitude{n}\", f\"Description{n}\"]\n",
    "        ]\n",
    "        temp.columns = [\"Frequency\", \"Latitude\", \"Longitude\", \"Description\"]\n",
    "        new = pd.concat([new, temp], ignore_index=True)\n",
    "\n",
    "    new = new.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "    for c, (left, right) in zip(\n",
    "        [(x & y & ~z), (x & ~y & z), (~x & y & z)],\n",
    "        [(\"_x\", \"_y\"), (\"_x\", \"\"), (\"_y\", \"\")],\n",
    "    ):\n",
    "        for row in abc[c].itertuples():\n",
    "            d = geodesic(\n",
    "                (getattr(row, f\"Latitude{left}\"), getattr(row, f\"Longitude{left}\")),\n",
    "                (getattr(row, f\"Latitude{right}\"), getattr(row, f\"Longitude{right}\")),\n",
    "            ).m\n",
    "            if d < dist:\n",
    "                f = row.Frequency\n",
    "                lat = (\n",
    "                    getattr(row, f\"Latitude{left}\") + getattr(row, f\"Latitude{right}\")\n",
    "                ) / 2\n",
    "                lon = (\n",
    "                    getattr(row, f\"Longitude{left}\") + getattr(row, f\"Longitude{right}\")\n",
    "                ) / 2\n",
    "                d = (\n",
    "                    getattr(row, f\"Description{left}\")\n",
    "                    + \" | \"\n",
    "                    + getattr(row, f\"Description{right}\")\n",
    "                )\n",
    "                new = pd.concat(\n",
    "                    [\n",
    "                        new,\n",
    "                        pd.DataFrame(\n",
    "                            [[f, lat, lon, d]],\n",
    "                            columns=[\n",
    "                                \"Frequency\",\n",
    "                                \"Latitude\",\n",
    "                                \"Longitude\",\n",
    "                                \"Description\",\n",
    "                            ],\n",
    "                        ),\n",
    "                    ],\n",
    "                    ignore_index=True,\n",
    "                )\n",
    "            else:\n",
    "                l = (\n",
    "                    abc.loc[\n",
    "                        row.Index,\n",
    "                        [\n",
    "                            \"Frequency\",\n",
    "                            f\"Latitude{left}\",\n",
    "                            f\"Longitude{left}\",\n",
    "                            f\"Description{left}\",\n",
    "                        ],\n",
    "                    ]\n",
    "                    .to_frame()\n",
    "                    .T\n",
    "                )\n",
    "                l.columns = new.columns\n",
    "                r = (\n",
    "                    abc.loc[\n",
    "                        row.Index,\n",
    "                        [\n",
    "                            \"Frequency\",\n",
    "                            f\"Latitude{right}\",\n",
    "                            f\"Longitude{right}\",\n",
    "                            f\"Description{right}\",\n",
    "                        ],\n",
    "                    ]\n",
    "                    .to_frame()\n",
    "                    .T\n",
    "                )\n",
    "                r.columns = new.columns\n",
    "                new = pd.concat([new, l, r], ignore_index=True)\n",
    "\n",
    "    new = new.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
    "\n",
    "    for row in abc[x & y & z].itertuples():\n",
    "        d1 = geodesic(\n",
    "            (getattr(row, \"Latitude_x\"), getattr(row, \"Longitude_x\")),\n",
    "            (getattr(row, \"Latitude_y\"), getattr(row, \"Longitude_y\")),\n",
    "        ).m\n",
    "        d2 = geodesic(\n",
    "            (getattr(row, \"Latitude_x\"), getattr(row, \"Longitude_x\")),\n",
    "            (getattr(row, \"Latitude\"), getattr(row, \"Longitude\")),\n",
    "        ).m\n",
    "        d3 = geodesic(\n",
    "            (getattr(row, \"Latitude_y\"), getattr(row, \"Longitude_y\")),\n",
    "            (getattr(row, \"Latitude\"), getattr(row, \"Longitude\")),\n",
    "        ).m\n",
    "        if d1 < dist and d2 < dist and d3 < dist:\n",
    "            f = row.Frequency\n",
    "            lat = (row.Latitude_x + row.Latitude_y + row.Latitude) / 3\n",
    "            lon = (row.Longitude_x + row.Longitude_y + row.Longitude) / 3\n",
    "            d = \" | \".join([row.Description_x, row.Description_y, row.Description])\n",
    "            new = pd.concat(\n",
    "                [\n",
    "                    new,\n",
    "                    pd.DataFrame(\n",
    "                        [[f, lat, lon, d]],\n",
    "                        columns=[\"Frequency\", \"Latitude\", \"Longitude\", \"Description\"],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        elif d1 < dist and d2 < dist:\n",
    "            f = row.Frequency\n",
    "            lat = (row.Latitude_x + row.Latitude_y) / 2\n",
    "            lon = (row.Longitude_x + row.Longitude_y) / 2\n",
    "            d = \" | \".join([row.Description_x, row.Description_y])\n",
    "            new = pd.concat(\n",
    "                [\n",
    "                    new,\n",
    "                    pd.DataFrame(\n",
    "                        [[f, lat, lon, d]],\n",
    "                        columns=[\"Frequency\", \"Latitude\", \"Longitude\", \"Description\"],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        elif d1 < dist and d3 < dist:\n",
    "            f = row.Frequency\n",
    "            lat = (row.Latitude_x + row.Latitude) / 2\n",
    "            lon = (row.Longitude_x + row.Longitude) / 2\n",
    "            d = \" | \".join([row.Description_x, row.Description])\n",
    "            new = pd.concat(\n",
    "                [\n",
    "                    new,\n",
    "                    pd.DataFrame(\n",
    "                        [[f, lat, lon, d]],\n",
    "                        columns=[\"Frequency\", \"Latitude\", \"Longitude\", \"Description\"],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        elif d2 < dist and d3 < dist:\n",
    "            f = row.Frequency\n",
    "            lat = (row.Latitude_y + row.Latitude) / 2\n",
    "            lon = (row.Longitude_y + row.Longitude) / 2\n",
    "            d = \" | \".join([row.Description_y, row.Description])\n",
    "            new = pd.concat(\n",
    "                [\n",
    "                    new,\n",
    "                    pd.DataFrame(\n",
    "                        [[f, lat, lon, d]],\n",
    "                        columns=[\"Frequency\", \"Latitude\", \"Longitude\", \"Description\"],\n",
    "                    ),\n",
    "                ],\n",
    "                ignore_index=True,\n",
    "            )\n",
    "        else:\n",
    "            l = (\n",
    "                abc.loc[\n",
    "                    row.Index,\n",
    "                    [\"Frequency\", \"Latitude_x\", \"Longitude_x\", \"Description_x\"],\n",
    "                ]\n",
    "                .to_frame()\n",
    "                .T\n",
    "            )\n",
    "            l.columns = new.columns\n",
    "            r = (\n",
    "                abc.loc[\n",
    "                    row.Index,\n",
    "                    [\"Frequency\", \"Latitude_y\", \"Longitude_y\", \"Description_y\"],\n",
    "                ]\n",
    "                .to_frame()\n",
    "                .T\n",
    "            )\n",
    "            r.columns = new.columns\n",
    "            s = (\n",
    "                abc.loc[\n",
    "                    row.Index, [\"Frequency\", \"Latitude\", \"Longitude\", \"Description\"]\n",
    "                ]\n",
    "                .to_frame()\n",
    "                .T\n",
    "            )\n",
    "            s.columns = new.columns\n",
    "            new = pd.concat([new, l, r, s], ignore_index=True)\n",
    "\n",
    "    new = new.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
    "    new[\"Service\"] = -1\n",
    "    new[\"Station\"] = -1\n",
    "    return new\n",
    "\n",
    "\n",
    "def read_aero(pasta, up_icao=False, up_pmec=False, up_geo=False):\n",
    "    icao = read_icao(pasta, up_icao)\n",
    "    pmec = read_pmec(pasta, up_pmec)\n",
    "    geo = read_geo(pasta, up_geo)\n",
    "    icao[\"Description\"] = icao.Description.astype(\"string\")\n",
    "    pmec[\"Description\"] = pmec.Description.astype(\"string\")\n",
    "    geo[\"Description\"] = geo.Description.astype(\"string\")\n",
    "    common = _merge_common(icao, pmec, geo)\n",
    "    new = _merge_new(icao, pmec, geo)\n",
    "    return common, new\n",
    "\n",
    "\n",
    "def merge_aero(df, common, new):\n",
    "    \"\"\"Mescla a base da Anatel com as tabelas retiradas da Aeron√°utica\"\"\"\n",
    "    common = common.loc[:, [\"Frequency\", \"Description\", \"Service\", \"Station\"]]\n",
    "    df[\"Description\"] = df.Description.astype(\"string\")\n",
    "    df = pd.merge(df, common, on=[\"Frequency\", \"Service\", \"Station\"], how=\"left\")\n",
    "    df.loc[df.Description_y.notna(), \"Description_x\"] = (\n",
    "        df.loc[df.Description_y.notna(), \"Description_x\"]\n",
    "        + \" | \"\n",
    "        + df.loc[df.Description_y.notna(), \"Description_y\"]\n",
    "    )\n",
    "    df.drop(columns=[\"Description_y\"], inplace=True)\n",
    "    df.rename(columns={\"Description_x\": \"Description\"}, inplace=True)\n",
    "    new['Class'] = '-1'\n",
    "    new['BW'] = '-1'\n",
    "    return (\n",
    "        pd.concat([df, new], ignore_index=True)\n",
    "        .sort_values(\"Frequency\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "def update_base(pasta, up_stel=False, up_radcom=False, up_mosaico=False, up_icao=False):\n",
    "    \"\"\"Wrapper que atualiza opcionalmente l√™ e atualiza as tr√™s bases indicadas anteriormente, as combina e salva o arquivo consolidado na pasta `pasta`\"\"\"\n",
    "    stel = read_stel(pasta, up_stel).loc[:, TELECOM]\n",
    "    radcom = read_radcom(pasta, up_radcom)\n",
    "    mosaico = read_mosaico(pasta, up_mosaico)\n",
    "    radcom[\"Num_Servi√ßo\"] = \"231\"\n",
    "    radcom[\"Status\"] = \"RADCOM\"\n",
    "    radcom[\"Classe_Emiss√£o\"] = \"\"\n",
    "    radcom[\"Largura_Emiss√£o\"] = BW_MAP['231']\n",
    "    filtro = radcom.Fase.notna() & radcom.Situa√ß√£o.notna()\n",
    "    radcom.loc[filtro, \"Classe\"] = radcom.loc[filtro, \"Fase\"].astype(\"string\") + '-' + radcom.loc[filtro, \"Situa√ß√£o\"].astype(\"string\")\n",
    "    radcom[\"Entidade\"] = radcom.Entidade.str.rstrip().str.lstrip()\n",
    "    radcom[\"Num_Ato\"] = \"-1\"\n",
    "    radcom[\"Data_Ato\"] = \"\"\n",
    "    radcom[\"Validade_RF\"] = \"\"\n",
    "    radcom[\"Fonte\"] = \"SRD\"\n",
    "    radcom = df_optimize(radcom, exclude=[\"Frequ√™ncia\"])\n",
    "    stel[\"Status\"] = \"L\"\n",
    "    stel[\"Num_Ato\"] = \"-1\"\n",
    "    stel[\"Data_Ato\"] = \"\"\n",
    "    stel[\"Entidade\"] = stel.Entidade.str.rstrip().str.lstrip()\n",
    "    stel[\"Fonte\"] = \"STEL\"\n",
    "    stel = df_optimize(stel, exclude=[\"Frequ√™ncia\"])\n",
    "    mosaico[\"Fonte\"] = \"MOS\"\n",
    "    mosaico[\"Classe_Emiss√£o\"] = \"\"\n",
    "    mosaico[\"Largura_Emiss√£o\"] = mosaico.Num_Servi√ßo.map(BW_MAP)\n",
    "    mosaico = mosaico.loc[:, RADIODIFUSAO]\n",
    "    mosaico = df_optimize(mosaico, exclude=[\"Frequ√™ncia\"])\n",
    "    rd = (\n",
    "        pd.concat([mosaico, radcom, stel])\n",
    "        .sort_values(\"Frequ√™ncia\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    rd[\"Num_Servi√ßo\"] = rd.Num_Servi√ßo.astype(\"int\")\n",
    "    rd = df_optimize(rd, exclude=[\"Frequ√™ncia\"])\n",
    "    rd = rd.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
    "    rd['BW(kHz)'] = rd.Largura_Emiss√£o.apply(parse_bw)\n",
    "    console.print(\":trophy: [green]Base Consolidada. Salvando os arquivos...\")\n",
    "    try:\n",
    "        rd.to_feather(f\"{pasta}/base.fth\")\n",
    "    except ArrowInvalid:\n",
    "        Path(f\"{pasta}/base.fth\").unlink()\n",
    "        with pd.ExcelWriter(f\"{pasta}/base.xlsx\") as workbook:\n",
    "            rd.to_excel(workbook, sheet_name=\"Sheet1\", engine=\"openpyxl\", index=False)\n",
    "    return rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fun√ß√µes de Leitura das diversas bases\n",
    "A presente biblioteca utiliza tr√™s bases de dados: \n",
    "* `STEL` - Servi√ßos Privados de Telecomunica√ß√µes\n",
    "* `RADCOM` - Servi√ßo de Radiodifus√£o comunit√°ria\n",
    "* `MOSAICO` - Demais servi√ßos de Radiodifus√£o e paulatinamente tamb√©m adicionado servi√ßos privados\n",
    "\n",
    "As fun√ß√µes a seguir s√£o para leitura dos arquivos locais baixados e processados dessas bases, opcionalmente esses arquivos podem ser atualizados antes de serem lidos passando o argumento `update = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_stel(pasta, update=False):\n",
    "    \"\"\"L√™ o banco de dados salvo localmente do STEL. Opcionalmente o atualiza pelo Banco de Dados ANATELBDRO01 caso `update = True` ou n√£o exista o arquivo local\"\"\"\n",
    "    if update:\n",
    "        update_stel(pasta)\n",
    "    file = Path(f\"{pasta}/stel.fth\")\n",
    "    try:\n",
    "        stel = pd.read_feather(file)\n",
    "    except (ArrowInvalid, FileNotFoundError):\n",
    "        file = Path(f\"{pasta}/stel.xlsx\")\n",
    "        try:\n",
    "            stel = pd.read_excel(file, engine=\"openpyxl\")\n",
    "        except FileNotFoundError:\n",
    "            read_stel(pasta, True)\n",
    "\n",
    "    return stel\n",
    "\n",
    "\n",
    "def read_radcom(pasta, update=False):\n",
    "    \"\"\"L√™ o banco de dados salvo localmente de RADCOM. Opcionalmente o atualiza pelo Banco de Dados ANATELBDRO01 caso `update = True` ou n√£o exista o arquivo local\"\"\"\n",
    "    if update:\n",
    "        update_radcom(pasta)\n",
    "    file = Path(f\"{pasta}/radcom.fth\")\n",
    "    try:\n",
    "        radcom = pd.read_feather(file)\n",
    "    except (ArrowInvalid, FileNotFoundError):\n",
    "        file = Path(f\"{pasta}/radcom.xlsx\")\n",
    "        try:\n",
    "            radcom = pd.read_excel(file, engine=\"openpyxl\")\n",
    "        except FileNotFoundError:\n",
    "            read_radcom(pasta, True)\n",
    "    return radcom\n",
    "\n",
    "\n",
    "def read_mosaico(pasta, update=False):\n",
    "    \"\"\"L√™ o banco de dados salvo localmente do MOSAICO. Opcionalmente o atualiza antes da leitura baixando os diversos arquivos dispon√≠veis na interface web p√∫blica\"\"\"\n",
    "    if update:\n",
    "        update_mosaico(pasta)\n",
    "    file = Path(f\"{pasta}/mosaico.fth\")\n",
    "    try:\n",
    "        df = pd.read_feather(file)\n",
    "    except (ArrowInvalid, FileNotFoundError):\n",
    "        file = Path(f\"{pasta}/mosaico.xlsx\")\n",
    "        try:\n",
    "            df = pd.read_excel(file)\n",
    "        except FileNotFoundError:\n",
    "            return read_mosaico(pasta, update=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def read_icao(pasta, update=False):\n",
    "    \"\"\"L√™ a base de dados do Frequency Finder e Canaliza√ß√£o VOR/ILS/DME\"\"\"\n",
    "    if update:\n",
    "        # TODO: atualizar a base de dados do Frequency Finder e Canaliza√ß√£o VOR/ILS/DME\n",
    "        # update_icao(pasta)\n",
    "        raise NotImplementedError(\n",
    "            \"Atualizar da base de dados do Frequency Finder e Canaliza√ß√£o VOR/ILS/DME n√£o implementado\"\n",
    "        )\n",
    "    file = Path(f\"{pasta}/IcaoDB.fth\")\n",
    "    try:\n",
    "        icao = pd.read_feather(file)\n",
    "    except (ArrowInvalid, FileNotFoundError):\n",
    "        file = Path(f\"{pasta}/IcaoDB.xlsx\")\n",
    "        try:\n",
    "            icao = pd.read_excel(file, sheet_name=\"DataBase\", engine=\"openpyxl\")\n",
    "        except FileNotFoundError:\n",
    "            return read_icao(pasta, update=True)\n",
    "    return df_optimize(icao, exclude=[\"Frequency\"])\n",
    "\n",
    "\n",
    "def read_pmec(pasta, update=False):\n",
    "    \"\"\"Fontes da informa√ß√£o: AISWEB, REDEMET, Of√≠cio n¬∫ 2/SSARP/14410 e Canaliza√ß√£o VOR/ILS/DME.\"\"\"\n",
    "    if update:\n",
    "        # TODO: Atualizar a base de dados do AISWEB, REDEMET, Of√≠cio n¬∫ 2/SSARP/14410 e Canaliza√ß√£o VOR/ILS/DME\n",
    "        # update_pmec(pasta)\n",
    "        raise NotImplementedError(\n",
    "            \"Atualizar da base de dados do Frequency Finder e Canaliza√ß√£o VOR/ILS/DME n√£o implementado\"\n",
    "        )\n",
    "    file = Path(f\"{pasta}/PmecDB.fth\")\n",
    "    try:\n",
    "        df = pd.read_feather(file)\n",
    "    except (ArrowInvalid, FileNotFoundError):\n",
    "        file = Path(f\"{pasta}/PmecDB.xlsx\")\n",
    "        try:\n",
    "            df = pd.read_excel(file, sheet_name=\"DataBase\", engine=\"openpyxl\")\n",
    "        except FileNotFoundError:\n",
    "            return read_icao(pasta, update=True)\n",
    "    return df_optimize(df, exclude=[\"Frequency\"])\n",
    "\n",
    "\n",
    "def read_geo(pasta, update=False):\n",
    "    \"\"\"Fontes da informa√ß√£o: AISWEB, REDEMET, Of√≠cio n¬∫ 2/SSARP/14410 e Canaliza√ß√£o VOR/ILS/DME.\"\"\"\n",
    "    if update:\n",
    "        # TODO: Atualizar a base de dados do GEOAISWEB\n",
    "        # update_geo(pasta)\n",
    "        raise NotImplementedError(\n",
    "            \"Atualizar da base de dados do Frequency Finder e Canaliza√ß√£o VOR/ILS/DME n√£o implementado\"\n",
    "        )\n",
    "    file = Path(f\"{pasta}/GeoAiswebDB.fth\")\n",
    "    try:\n",
    "        df = pd.read_feather(file)\n",
    "    except (ArrowInvalid, FileNotFoundError):\n",
    "        file = Path(f\"{pasta}/GeoAiswebDB.xlsx\")\n",
    "        try:\n",
    "            df = pd.read_excel(file, sheet_name=\"DataBase\", engine=\"openpyxl\")\n",
    "        except FileNotFoundError:\n",
    "            return read_icao(pasta, update=True)\n",
    "    return df_optimize(df, exclude=[\"Frequency\"])\n",
    "\n",
    "\n",
    "def read_base(pasta, up_stel=False, up_radcom=False, up_mosaico=False, up_icao=False):\n",
    "    \"\"\"Wrapper que combina a chamada das tr√™s fun√ß√µes de leitura do banco e opcionalmente √© poss√≠vel atualiz√°-las antes da leitura\"\"\"\n",
    "    if any([up_stel, up_radcom, up_mosaico, up_icao]):\n",
    "        update_base(pasta, up_stel, up_radcom, up_mosaico, up_icao)\n",
    "    file = Path(f\"{pasta}/base.fth\")\n",
    "    try:\n",
    "        df = pd.read_feather(file)\n",
    "    except (ArrowInvalid, FileNotFoundError):\n",
    "        file = Path(f\"{pasta}/base.xlsx\")\n",
    "        try:\n",
    "            df = pd.read_excel(file, engine=\"openpyxl\")\n",
    "        except FileNotFoundError:\n",
    "            df = update_base(pasta, True, True, True)\n",
    "    return df_optimize(df, exclude=[\"Frequ√™ncia\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "stel = read_stel(pasta).loc[:, TELECOM]\n",
    "radcom = read_radcom(pasta)\n",
    "mosaico = read_mosaico(pasta)\n",
    "radcom[\"Num_Servi√ßo\"] = \"231\"\n",
    "radcom[\"Status\"] = \"RADCOM\"\n",
    "radcom[\"Classe_Emiss√£o\"] = \"\"\n",
    "radcom[\"Largura_Emiss√£o\"] = BW_MAP['231']\n",
    "filtro = radcom.Fase.notna() & radcom.Situa√ß√£o.notna()\n",
    "radcom.loc[filtro, \"Classe\"] = radcom.loc[filtro, \"Fase\"].astype(\"string\") + '-' + radcom.loc[filtro, \"Situa√ß√£o\"].astype(\"string\")\n",
    "radcom[\"Entidade\"] = radcom.Entidade.str.rstrip().str.lstrip()\n",
    "radcom[\"Num_Ato\"] = \"-1\"\n",
    "radcom[\"Data_Ato\"] = \"\"\n",
    "radcom[\"Validade_RF\"] = \"\"\n",
    "radcom[\"Fonte\"] = \"SRD\"\n",
    "radcom = df_optimize(radcom, exclude=[\"Frequ√™ncia\"])\n",
    "stel[\"Status\"] = \"L\"\n",
    "stel[\"Num_Ato\"] = \"-1\"\n",
    "stel[\"Data_Ato\"] = \"\"\n",
    "stel[\"Entidade\"] = stel.Entidade.str.rstrip().str.lstrip()\n",
    "stel[\"Fonte\"] = \"STEL\"\n",
    "stel = df_optimize(stel, exclude=[\"Frequ√™ncia\"])\n",
    "mosaico[\"Fonte\"] = \"MOS\"\n",
    "mosaico[\"Classe_Emiss√£o\"] = \"\"\n",
    "mosaico[\"Largura_Emiss√£o\"] = mosaico.Num_Servi√ßo.map(BW_MAP)\n",
    "mosaico = mosaico.loc[:, RADIODIFUSAO]\n",
    "mosaico = df_optimize(mosaico, exclude=[\"Frequ√™ncia\"])\n",
    "rd = (\n",
    "    pd.concat([mosaico, radcom, stel])\n",
    "    .sort_values(\"Frequ√™ncia\")\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "rd[\"Num_Servi√ßo\"] = rd.Num_Servi√ßo.astype(\"int\")\n",
    "rd = df_optimize(rd, exclude=[\"Frequ√™ncia\"])\n",
    "rd = rd.drop_duplicates(keep=\"first\").reset_index(drop=True)\n",
    "rd['BW(kHz)'] = rd.Largura_Emiss√£o.apply(parse_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">üèÜ <span style=\"color: #008000; text-decoration-color: #008000\">Base Consolidada. Salvando os arquivos...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "üèÜ \u001b[32mBase Consolidada. Salvando os arquivos\u001b[0m\u001b[32m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Frequ√™ncia</th>\n",
       "      <th>Num_Servi√ßo</th>\n",
       "      <th>Status</th>\n",
       "      <th>Classe</th>\n",
       "      <th>Entidade</th>\n",
       "      <th>Fistel</th>\n",
       "      <th>N√∫mero_da_Esta√ß√£o</th>\n",
       "      <th>Munic√≠pio</th>\n",
       "      <th>UF</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>...</th>\n",
       "      <th>Num_Ato</th>\n",
       "      <th>Data_Ato</th>\n",
       "      <th>Classe_Emiss√£o</th>\n",
       "      <th>Largura_Emiss√£o</th>\n",
       "      <th>Fase</th>\n",
       "      <th>Situa√ß√£o</th>\n",
       "      <th>CNPJ</th>\n",
       "      <th>Unidade</th>\n",
       "      <th>Fonte</th>\n",
       "      <th>BW(kHz)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0280</td>\n",
       "      <td>19</td>\n",
       "      <td>L</td>\n",
       "      <td>OP</td>\n",
       "      <td>FURNAS CENTRAIS ELETRICAS S A</td>\n",
       "      <td>01030052263</td>\n",
       "      <td>1557670</td>\n",
       "      <td>Nova Igua√ßu</td>\n",
       "      <td>RJ</td>\n",
       "      <td>-22.662777</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>J9E</td>\n",
       "      <td>8K00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STEL</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0285</td>\n",
       "      <td>19</td>\n",
       "      <td>L</td>\n",
       "      <td>OP</td>\n",
       "      <td>COMPANHIA DE GERA√á√ÉO E TRANSMISS√ÉO DE ENERGIA ...</td>\n",
       "      <td>50420217282</td>\n",
       "      <td>1494686</td>\n",
       "      <td>Joinville</td>\n",
       "      <td>SC</td>\n",
       "      <td>-26.292500</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>R3E</td>\n",
       "      <td>2K50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STEL</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0300</td>\n",
       "      <td>19</td>\n",
       "      <td>L</td>\n",
       "      <td>OP</td>\n",
       "      <td>FURNAS CENTRAIS ELETRICAS S A</td>\n",
       "      <td>01030052263</td>\n",
       "      <td>859761</td>\n",
       "      <td>Rio de Janeiro</td>\n",
       "      <td>RJ</td>\n",
       "      <td>-22.926666</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>J3E</td>\n",
       "      <td>500H</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STEL</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0300</td>\n",
       "      <td>19</td>\n",
       "      <td>L</td>\n",
       "      <td>OP</td>\n",
       "      <td>FURNAS CENTRAIS ELETRICAS S A</td>\n",
       "      <td>01030052263</td>\n",
       "      <td>859966</td>\n",
       "      <td>Arapor√£</td>\n",
       "      <td>MG</td>\n",
       "      <td>-18.410000</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>J3E</td>\n",
       "      <td>1K00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STEL</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0300</td>\n",
       "      <td>19</td>\n",
       "      <td>L</td>\n",
       "      <td>OP</td>\n",
       "      <td>FURNAS CENTRAIS ELETRICAS S A</td>\n",
       "      <td>01030052263</td>\n",
       "      <td>859753</td>\n",
       "      <td>Campinas</td>\n",
       "      <td>SP</td>\n",
       "      <td>-22.774166</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>J3E</td>\n",
       "      <td>1K00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STEL</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883625</th>\n",
       "      <td>85469.0000</td>\n",
       "      <td>19</td>\n",
       "      <td>L</td>\n",
       "      <td>FX</td>\n",
       "      <td>TIM S A</td>\n",
       "      <td>50417425295</td>\n",
       "      <td>1009125734</td>\n",
       "      <td>Par√° de Minas</td>\n",
       "      <td>MG</td>\n",
       "      <td>-19.857389</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>Q7W</td>\n",
       "      <td>62M5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STEL</td>\n",
       "      <td>62500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883626</th>\n",
       "      <td>85469.0000</td>\n",
       "      <td>19</td>\n",
       "      <td>L</td>\n",
       "      <td>FX</td>\n",
       "      <td>TIM S A</td>\n",
       "      <td>50417425295</td>\n",
       "      <td>1008775875</td>\n",
       "      <td>Fortaleza</td>\n",
       "      <td>CE</td>\n",
       "      <td>-3.734861</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>Q7W</td>\n",
       "      <td>62M5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STEL</td>\n",
       "      <td>62500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883627</th>\n",
       "      <td>85469.0000</td>\n",
       "      <td>19</td>\n",
       "      <td>L</td>\n",
       "      <td>FX</td>\n",
       "      <td>TIM S A</td>\n",
       "      <td>50417425295</td>\n",
       "      <td>1009131726</td>\n",
       "      <td>Caruaru</td>\n",
       "      <td>PE</td>\n",
       "      <td>-8.266858</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>Q7W</td>\n",
       "      <td>62M5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STEL</td>\n",
       "      <td>62500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883628</th>\n",
       "      <td>85469.0000</td>\n",
       "      <td>19</td>\n",
       "      <td>L</td>\n",
       "      <td>FX</td>\n",
       "      <td>TIM S A</td>\n",
       "      <td>50417425295</td>\n",
       "      <td>1005059940</td>\n",
       "      <td>Rio Branco</td>\n",
       "      <td>AC</td>\n",
       "      <td>-9.937445</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>Q7W</td>\n",
       "      <td>62M5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STEL</td>\n",
       "      <td>62500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883629</th>\n",
       "      <td>85469.0000</td>\n",
       "      <td>19</td>\n",
       "      <td>L</td>\n",
       "      <td>FX</td>\n",
       "      <td>TIM S A</td>\n",
       "      <td>50417425295</td>\n",
       "      <td>1009786951</td>\n",
       "      <td>Ananindeua</td>\n",
       "      <td>PA</td>\n",
       "      <td>-1.358931</td>\n",
       "      <td>...</td>\n",
       "      <td>-1</td>\n",
       "      <td></td>\n",
       "      <td>Q7W</td>\n",
       "      <td>750M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>STEL</td>\n",
       "      <td>750000.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>883630 rows √ó 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Frequ√™ncia  Num_Servi√ßo Status Classe  \\\n",
       "0           0.0280           19      L     OP   \n",
       "1           0.0285           19      L     OP   \n",
       "2           0.0300           19      L     OP   \n",
       "3           0.0300           19      L     OP   \n",
       "4           0.0300           19      L     OP   \n",
       "...            ...          ...    ...    ...   \n",
       "883625  85469.0000           19      L     FX   \n",
       "883626  85469.0000           19      L     FX   \n",
       "883627  85469.0000           19      L     FX   \n",
       "883628  85469.0000           19      L     FX   \n",
       "883629  85469.0000           19      L     FX   \n",
       "\n",
       "                                                 Entidade       Fistel  \\\n",
       "0                           FURNAS CENTRAIS ELETRICAS S A  01030052263   \n",
       "1       COMPANHIA DE GERA√á√ÉO E TRANSMISS√ÉO DE ENERGIA ...  50420217282   \n",
       "2                           FURNAS CENTRAIS ELETRICAS S A  01030052263   \n",
       "3                           FURNAS CENTRAIS ELETRICAS S A  01030052263   \n",
       "4                           FURNAS CENTRAIS ELETRICAS S A  01030052263   \n",
       "...                                                   ...          ...   \n",
       "883625                                            TIM S A  50417425295   \n",
       "883626                                            TIM S A  50417425295   \n",
       "883627                                            TIM S A  50417425295   \n",
       "883628                                            TIM S A  50417425295   \n",
       "883629                                            TIM S A  50417425295   \n",
       "\n",
       "        N√∫mero_da_Esta√ß√£o       Munic√≠pio  UF   Latitude  ...  Num_Ato  \\\n",
       "0                 1557670     Nova Igua√ßu  RJ -22.662777  ...       -1   \n",
       "1                 1494686       Joinville  SC -26.292500  ...       -1   \n",
       "2                  859761  Rio de Janeiro  RJ -22.926666  ...       -1   \n",
       "3                  859966         Arapor√£  MG -18.410000  ...       -1   \n",
       "4                  859753        Campinas  SP -22.774166  ...       -1   \n",
       "...                   ...             ...  ..        ...  ...      ...   \n",
       "883625         1009125734   Par√° de Minas  MG -19.857389  ...       -1   \n",
       "883626         1008775875       Fortaleza  CE  -3.734861  ...       -1   \n",
       "883627         1009131726         Caruaru  PE  -8.266858  ...       -1   \n",
       "883628         1005059940      Rio Branco  AC  -9.937445  ...       -1   \n",
       "883629         1009786951      Ananindeua  PA  -1.358931  ...       -1   \n",
       "\n",
       "       Data_Ato Classe_Emiss√£o Largura_Emiss√£o Fase Situa√ß√£o  CNPJ Unidade  \\\n",
       "0                          J9E            8K00  NaN      NaN  <NA>     NaN   \n",
       "1                          R3E            2K50  NaN      NaN  <NA>     NaN   \n",
       "2                          J3E            500H  NaN      NaN  <NA>     NaN   \n",
       "3                          J3E            1K00  NaN      NaN  <NA>     NaN   \n",
       "4                          J3E            1K00  NaN      NaN  <NA>     NaN   \n",
       "...         ...            ...             ...  ...      ...   ...     ...   \n",
       "883625                     Q7W            62M5  NaN      NaN  <NA>     NaN   \n",
       "883626                     Q7W            62M5  NaN      NaN  <NA>     NaN   \n",
       "883627                     Q7W            62M5  NaN      NaN  <NA>     NaN   \n",
       "883628                     Q7W            62M5  NaN      NaN  <NA>     NaN   \n",
       "883629                     Q7W            750M  NaN      NaN  <NA>     NaN   \n",
       "\n",
       "       Fonte   BW(kHz)  \n",
       "0       STEL       8.0  \n",
       "1       STEL       2.5  \n",
       "2       STEL       0.5  \n",
       "3       STEL       1.0  \n",
       "4       STEL       1.0  \n",
       "...      ...       ...  \n",
       "883625  STEL   62500.0  \n",
       "883626  STEL   62500.0  \n",
       "883627  STEL   62500.0  \n",
       "883628  STEL   62500.0  \n",
       "883629  STEL  750000.0  \n",
       "\n",
       "[883630 rows x 22 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_base(pasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted constants.ipynb.\n",
      "Converted filter.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted queries.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script; notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anateldb]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
